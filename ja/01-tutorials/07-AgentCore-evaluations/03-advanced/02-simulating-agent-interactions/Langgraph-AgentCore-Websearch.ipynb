{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ActorSimulator ã‚’ä½¿ç”¨ã—ãŸ AgentCore ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ - Web æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\n",
    "\n",
    "**ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:** AgentCore ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— â†’ DatasetGenerator â†’ ActorSimulator â†’ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘¼ã³å‡ºã— â†’ AgentCore ãŒ CloudWatch çµŒç”±ã§è©•ä¾¡\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯:\n",
    "1. `eval_config.py` ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿\n",
    "2. Web æ¤œç´¢å“è³ªã®ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ  LLM-as-a-Judge è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½œæˆ\n",
    "3. ãƒ“ãƒ«ãƒˆã‚¤ãƒ³ + ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ AgentCore ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "4. DatasetGenerator ã‚’ä½¿ç”¨ã—ã¦ Web æ¤œç´¢ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ç”Ÿæˆ\n",
    "5. ActorSimulator ã‚’å®Ÿè¡Œã—ã¦ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—\n",
    "6. AgentCore ãŒ CloudWatch çµŒç”±ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è‡ªå‹•çš„ã«ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from strands_evals import ActorSimulator, Case\n",
    "from strands_evals.generators import DatasetGenerator\n",
    "from lg_eval_config import *\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "print(\"Configuration loaded from eval_config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä½œæˆï¼ˆLLM-as-a-Judgeï¼‰\n",
    "\n",
    "Web æ¤œç´¢å“è³ªã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚ä»¥ä¸‹ã‚’å«ã¿ã¾ã™:\n",
    "- æ¤œç´¢çµæœã®é–¢é€£æ€§\n",
    "- æƒ…å ±åˆæˆã®å“è³ª\n",
    "- å‡ºå…¸ã®å¸°å±\n",
    "- æƒ…å ±ã®é®®åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_client = boto3.client(\n",
    "    'agentcore-evaluation-controlplane',\n",
    "    region_name=AWS_REGION,\n",
    ")\n",
    "\n",
    "# Create custom evaluator\n",
    "try:\n",
    "    custom_evaluator_response = evaluation_client.create_evaluator(\n",
    "        evaluatorName=CUSTOM_EVALUATOR_NAME,\n",
    "        level=\"TRACE\",\n",
    "        evaluatorConfig=CUSTOM_EVALUATOR_CONFIG\n",
    "    )\n",
    "    print(f\"âœ“ Created custom evaluator: {CUSTOM_EVALUATOR_NAME}\")\n",
    "    print(json.dumps(custom_evaluator_response, indent=2, default=str))\n",
    "    custom_evaluator_id = custom_evaluator_response['evaluatorId']\n",
    "except Exception as e:\n",
    "    if 'ResourceConflictException' in str(e) or 'already exists' in str(e):\n",
    "        print(f\"âš  Custom evaluator '{CUSTOM_EVALUATOR_NAME}' already exists, using existing one\")\n",
    "        # List evaluators to get the existing ID\n",
    "        list_response = evaluation_client.list_evaluators()\n",
    "        for evaluator in list_response.get('evaluators', []):\n",
    "            if evaluator['evaluatorName'] == CUSTOM_EVALUATOR_NAME:\n",
    "                custom_evaluator_id = evaluator['evaluatorId']\n",
    "                print(f\"âœ“ Found existing evaluator ID: {custom_evaluator_id}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"âœ— Error creating custom evaluator: {e}\")\n",
    "        raise\n",
    "\n",
    "create_config_response = evaluation_client.create_online_evaluation_config(\n",
    "    onlineEvaluationConfigName=\"custom_web_search_quality_evaluator\",\n",
    "    description=\"Integration test config\",\n",
    "    rule={\n",
    "        \"samplingConfig\": {\"samplingPercentage\": 100.0}\n",
    "    },\n",
    "    dataSourceConfig={\n",
    "        \"cloudWatchLogs\": {\n",
    "            \"logGroupNames\": [LOG_GROUP_NAME],\n",
    "            \"serviceNames\": [SERVICE_NAME]\n",
    "        }\n",
    "    },\n",
    "    evaluators=[{\"evaluatorId\":custom_evaluator_id}],\n",
    "    evaluationExecutionRoleArn=EVALUATION_ROLE_ARN,\n",
    "    enableOnCreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. AgentCore ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "ãƒ“ãƒ«ãƒˆã‚¤ãƒ³ã¨ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¸¡æ–¹ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’è¨­å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine builtin evaluators with custom evaluator\n",
    "all_evaluators = [{\"evaluatorId\": evaluator_id} for evaluator_id in EVALUATORS]\n",
    "\n",
    "print(f\"Total evaluators: {len(all_evaluators)} (13 builtin + 1 custom)\")\n",
    "\n",
    "create_config_response = evaluation_client.create_online_evaluation_config(\n",
    "    onlineEvaluationConfigName=EVAL_CONFIG_NAME,\n",
    "    description=EVAL_DESCRIPTION,\n",
    "    rule={\n",
    "        \"samplingConfig\": {\"samplingPercentage\": SAMPLING_PERCENTAGE},\n",
    "        \"sessionConfig\": {\"sessionTimeoutMinutes\": SESSION_TIMEOUT_MINUTES}\n",
    "    },\n",
    "    dataSourceConfig={\n",
    "        \"cloudWatchLogs\": {\n",
    "            \"logGroupNames\": [LOG_GROUP_NAME],\n",
    "            \"serviceNames\": [SERVICE_NAME]\n",
    "        }\n",
    "    },\n",
    "    evaluators=all_evaluators,\n",
    "    evaluationExecutionRoleArn=EVALUATION_ROLE_ARN,\n",
    "    enableOnCreate=True\n",
    ")\n",
    "\n",
    "config_id = create_config_response['onlineEvaluationConfigId']\n",
    "config_details = evaluation_client.get_online_evaluation_config(onlineEvaluationConfigId=config_id)\n",
    "\n",
    "print(f\"\\nâœ“ Created config: {config_id}\")\n",
    "print(f\"Status: {config_details['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. AgentCore Runtime ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_client = boto3.client('bedrock-agentcore', region_name=AWS_REGION)\n",
    "\n",
    "def invoke_agentcore(user_message, session_id=None):\n",
    "    \"\"\"\n",
    "    Invoke agent with session management.\n",
    "    \n",
    "    Args:\n",
    "        user_message: The prompt to send to the agent\n",
    "        session_id: Optional session ID for maintaining conversation context\n",
    "    \n",
    "    Returns:\n",
    "        Response text from the agent\n",
    "    \"\"\"\n",
    "    # Build the request parameters\n",
    "    request_params = {\n",
    "        'agentRuntimeArn': AGENT_ARN,\n",
    "        'qualifier': QUALIFIER,\n",
    "        'payload': json.dumps({\"prompt\": user_message})\n",
    "    }\n",
    "    \n",
    "    # Add session_id if provided\n",
    "    if session_id is not None:\n",
    "        request_params['runtimeSessionId'] = session_id\n",
    "    \n",
    "    boto3_response = agentcore_client.invoke_agent_runtime(**request_params)\n",
    "    \n",
    "    content = []\n",
    "    if \"text/event-stream\" in boto3_response.get(\"contentType\", \"\"):\n",
    "        for line in boto3_response[\"response\"].iter_lines(chunk_size=1):\n",
    "            if line:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line.startswith(\"data: \"):\n",
    "                    line = line[6:]\n",
    "                    content.append(line)\n",
    "    else:\n",
    "        events = []\n",
    "        for event in boto3_response.get(\"response\", []):\n",
    "            events.append(event)\n",
    "        if events:\n",
    "            content.append(json.loads(events[0].decode(\"utf-8\")))\n",
    "    \n",
    "    return \"\\n\".join(str(c) for c in content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DatasetGenerator[str, str](str, str)\n",
    "\n",
    "task_description = f\"\"\"\n",
    "Task: {AGENT_CAPABILITIES}\n",
    "Limitations: {AGENT_LIMITATIONS}\n",
    "Available tools: {', '.join(AGENT_TOOLS)}\n",
    "Complexity: {AGENT_COMPLEXITY}\n",
    "\"\"\"\n",
    "\n",
    "dataset = await generator.from_scratch_async(\n",
    "    topics=AGENT_TOPICS,\n",
    "    task_description=task_description,\n",
    "    num_cases=NUM_TEST_CASES\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(dataset.cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, case in enumerate(dataset.cases, 1):\n",
    "    print(f\"\\nCase {i}: {case.input}\")\n",
    "    print(f\"Expected: {case.expected_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. ã‚¿ã‚¹ã‚¯é–¢æ•°ã®å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_function(case: Case) -> str:\n",
    "    # Create a new session for this test case\n",
    "    session_id = str(uuid.uuid4())\n",
    "    print(f\"\\nğŸ†• Started new session: {session_id}\")\n",
    "    \n",
    "    user_sim = ActorSimulator.from_case_for_user_simulator(case=case, max_turns=MAX_TURNS)\n",
    "    \n",
    "    user_message = case.input\n",
    "    final_response = \"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test Case: {case.input}\")\n",
    "    print(f\"Expected: {case.expected_output}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    turn = 1\n",
    "    while user_sim.has_next():\n",
    "        print(f\"\\nTurn {turn}: {user_message}\")\n",
    "        agent_response = invoke_agentcore(user_message, session_id=session_id)\n",
    "        final_response = agent_response\n",
    "        print(f\"Agent: {agent_response[:200]}...\")\n",
    "        \n",
    "        user_result = user_sim.act(agent_response)\n",
    "        user_message = str(user_result.structured_output.message)\n",
    "        turn += 1\n",
    "    \n",
    "    print(f\"ğŸ”š Ended session: {session_id}\")\n",
    "    \n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 8. è©•ä¾¡ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, case in enumerate(dataset.cases, 1):\n",
    "    print(f\"\\n\\n{'#'*80}\")\n",
    "    print(f\"# Running Test Case {i}/{len(dataset.cases)}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    try:\n",
    "        response = task_function(case)\n",
    "        results.append({\n",
    "            \"case_number\": i,\n",
    "            \"input\": case.input,\n",
    "            \"expected\": case.expected_output,\n",
    "            \"actual\": response,\n",
    "            \"status\": \"success\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        results.append({\n",
    "            \"case_number\": i,\n",
    "            \"input\": case.input,\n",
    "            \"expected\": case.expected_output,\n",
    "            \"actual\": str(e),\n",
    "            \"status\": \"error\"\n",
    "        })\n",
    "\n",
    "print(f\"\\n\\nCompleted {len(results)} test cases\")\n",
    "print(f\"Successful: {sum(1 for r in results if r['status'] == 'success')}\")\n",
    "print(f\"Errors: {sum(1 for r in results if r['status'] == 'error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 9. çµæœã®è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®è©•ä¾¡ã§ã¯ã€Web æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä»¥ä¸‹ã§ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸ:\n",
    "\n",
    "**åˆè¨ˆ14ã®è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹:**\n",
    "- 13ã®ãƒ“ãƒ«ãƒˆã‚¤ãƒ³è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆCorrectnessã€Faithfulnessã€Helpfulnessã€Relevanceã€Concisenessã€Coherenceã€InstructionFollowingã€Refusalã€Harmfulnessã€Stereotypingã€GoalSuccessRateã€ToolSelectionAccuracyã€ToolParameterAccuracyï¼‰\n",
    "- 1ã¤ã®ã‚«ã‚¹ã‚¿ãƒ  LLM-as-a-Judge è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆWeb Search Qualityï¼‰\n",
    "\n",
    "ã™ã¹ã¦ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã¨è©•ä¾¡ã¯ CloudWatch ã«ã‚­ãƒ£ãƒ—ãƒãƒ£ã•ã‚Œã€AgentCore ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚\n",
    "\n",
    "# çµ‚äº†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
