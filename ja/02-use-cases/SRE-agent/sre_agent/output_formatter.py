#!/usr/bin/env python3

import logging
import os
from typing import Any, Dict, List, Optional

from .constants import SREConstants
from .llm_utils import create_llm_with_error_handling
from .prompt_loader import prompt_loader

# basicConfig ã§ãƒ­ã‚®ãƒ³ã‚°ã‚’è¨­å®š
logging.basicConfig(
    level=logging.INFO,  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ INFO ã«è¨­å®š
    # ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å®šç¾©
    format="%(asctime)s,p%(process)s,{%(filename)s:%(lineno)d},%(levelname)s,%(message)s",
)

logger = logging.getLogger(__name__)


class SREOutputFormatter:
    """SRE ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãª Markdown å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã€‚"""

    def __init__(self, llm_provider: Optional[str] = None):
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ç’°å¢ƒå¤‰æ•°ã€ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® bedrock ã‹ã‚‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
        self.llm_provider = llm_provider or os.getenv("LLM_PROVIDER", "bedrock")
        logger.info(
            f"SREOutputFormatter initialized with LLM provider: {self.llm_provider}"
        )

    def _create_llm(self, **kwargs):
        """æ”¹å–„ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å‚™ãˆãŸ LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚"""
        # å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼å›ºæœ‰ã®è¨­å®šã‚’å–å¾—ï¼ˆmax_tokens ã‚’å‰Šæ¸›ï¼‰
        formatter_config = SREConstants.get_output_formatter_config(
            self.llm_provider, **kwargs
        )
        logger.info(
            f"Creating LLM for output formatter - Provider: {self.llm_provider}, Max tokens: {formatter_config['max_tokens']}"
        )

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼å›ºæœ‰ã®è¨­å®šã§é›†ä¸­å‹ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨
        return create_llm_with_error_handling(
            self.llm_provider, max_tokens=formatter_config["max_tokens"], **kwargs
        )

    def _extract_steps_from_response(self, response: str) -> List[str]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ç•ªå·ä»˜ãã‚¹ãƒ†ãƒƒãƒ—ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"""
        if not response:
            return []

        steps = []
        lines = response.split("\n")

        for line in lines:
            line = line.strip()
            # ç•ªå·ä»˜ãã‚¹ãƒ†ãƒƒãƒ—ï¼ˆ1.ã€2. ãªã©ï¼‰ã¾ãŸã¯ç®‡æ¡æ›¸ãã‚’æ¢ã™
            if line and (
                line[0].isdigit() or line.startswith("-") or line.startswith("â€¢")
            ):
                steps.append(line)

        return steps

    def format_investigation_response(
        self,
        query: str,
        agent_results: Dict[str, Any],
        metadata: Dict[str, Any],
        plan: Optional[Dict[str, Any]] = None,
        user_preferences: Optional[List[Dict[str, Any]]] = None,
    ) -> str:
        """å®Œå…¨ãªèª¿æŸ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ãª Markdown ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™ã€‚"""

        # ä¸»è¦æƒ…å ±ã‚’æŠ½å‡º
        plan_info = plan or metadata.get("investigation_plan", {})
        current_step = metadata.get("plan_step", 0) + 1
        total_steps = len(plan_info.get("steps", []))

        output = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        output.append("# ğŸ” èª¿æŸ»çµæœ")
        output.append("")
        output.append(f"**Query:** {query}")
        output.append("")

        # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        executive_summary = self._generate_executive_summary(
            query, agent_results, metadata, user_preferences
        )
        if executive_summary:
            output.append(executive_summary)
            output.append("")

        # ä¸»è¦ç™ºè¦‹äº‹é …ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if agent_results:
            output.append("## ğŸ¯ ä¸»è¦ç™ºè¦‹äº‹é …")
            output.append("")

            for agent_name, result in agent_results.items():
                if not result or result == "No response provided":
                    continue

                agent_display = agent_name.replace("_", " ").title()
                output.append(f"### {agent_display}")

                # ãƒ©ãƒ³ãƒ–ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯ã‚¹ãƒ†ãƒƒãƒ—ã‚’æŠ½å‡º
                if (
                    "runbooks" in agent_name.lower()
                    or "operational" in agent_name.lower()
                ):
                    steps = self._extract_steps_from_response(result)
                    if steps:
                        output.append("")
                        output.append("**ç™ºè¦‹ã•ã‚ŒãŸãƒ©ãƒ³ãƒ–ãƒƒã‚¯ã‚¹ãƒ†ãƒƒãƒ—:**")
                        for step in steps:
                            # ã‚¹ãƒ†ãƒƒãƒ—ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ•´ç†
                            clean_step = step.strip()
                            if clean_step.startswith(
                                ("1.", "2.", "3.", "4.", "5.", "6.", "7.", "8.", "9.")
                            ):
                                output.append(f"{clean_step}")
                            else:
                                output.append(f"- {clean_step}")
                        output.append("")
                    else:
                        # ã‚¹ãƒ†ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º
                        output.append(f"- {result}")
                        output.append("")
                else:
                    # ãƒ©ãƒ³ãƒ–ãƒƒã‚¯ä»¥å¤–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ´åˆã¯å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º
                    output.append(f"- {result}")
                    output.append("")

        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if plan_info and current_step < total_steps:
            output.append("## ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
            output.append("")
            remaining_steps = plan_info.get("steps", [])[current_step:]
            for i, step in enumerate(remaining_steps, current_step + 1):
                output.append(f"{i}. {step}")
            output.append("")

        # èª¿æŸ»å®Œäº†
        if current_step >= total_steps:
            output.append("## âœ… èª¿æŸ»å®Œäº†")
            output.append("")
            output.append("è¨ˆç”»ã•ã‚ŒãŸã™ã¹ã¦ã®èª¿æŸ»ã‚¹ãƒ†ãƒƒãƒ—ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚")
            output.append("")

        return "\n".join(output)

    def _generate_executive_summary(
        self,
        query: str,
        agent_results: Dict[str, Any],
        metadata: Dict[str, Any],
        user_preferences: Optional[List[Dict[str, Any]]] = None,
    ) -> str:
        """èª¿æŸ»çµæœã® LLM åˆ†æã‚’ä½¿ç”¨ã—ã¦ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"""
        if not agent_results:
            return ""

        try:
            from langchain_core.messages import HumanMessage, SystemMessage

            # è¨­å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            llm = self._create_llm()

            # åˆ†æç”¨ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµæœã‚’æº–å‚™
            formatted_results = []
            for agent_name, result in agent_results.items():
                if result and result != "No response provided":
                    formatted_results.append(f"**{agent_name}:**\n{result}\n")

            results_text = "\n".join(formatted_results)

            # åˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
            if user_preferences:
                import json

                prefs_text = json.dumps(user_preferences, indent=2, default=str)
                results_text += f"\n\n**ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š:**\n{prefs_text}\n"

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt, user_prompt = prompt_loader.get_executive_summary_prompts(
                query=query, results_text=results_text
            )

            # ãƒ‡ãƒãƒƒã‚°ç”¨ã« LLM ã«é€ä¿¡ã•ã‚Œã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info("=== EXECUTIVE SUMMARY PROMPT LOGGING ===")
            logger.info(f"System Prompt Length: {len(system_prompt)} characters")
            logger.info(f"User Prompt Length: {len(user_prompt)} characters")
            if user_preferences:
                logger.info(
                    f"User preferences included in context: {len(user_preferences)} preference items"
                )
                logger.info(
                    f"User preferences preview: {str(user_preferences)[:200]}..."
                )
            else:
                logger.info(
                    "No user preferences provided to executive summary generation"
                )
            logger.info(f"User Prompt Content:\n{user_prompt}")
            logger.info("=== END EXECUTIVE SUMMARY PROMPT LOGGING ===")

            # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ]

            response = llm.invoke(messages)
            return str(response.content).strip()

        except Exception as e:
            logger.error(f"LLM ã§ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # LLM ãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚·ãƒ³ãƒ—ãƒ«ãªã‚µãƒãƒªãƒ¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._generate_fallback_summary(query, agent_results)

    def _generate_fallback_summary(
        self, query: str, agent_results: Dict[str, Any]
    ) -> str:
        """LLM ç”ŸæˆãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã€‚"""
        return """## ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ğŸ¯ ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
- **æ ¹æœ¬åŸå› **: èª¿æŸ»çµæœã«ã¯åˆ†æãŒå¿…è¦ã§ã™
- **å½±éŸ¿**: ã‚µãƒ¼ãƒ“ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- **é‡å¤§åº¦**: ä¸­

### âš¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. **å³æ™‚** (< 1æ™‚é–“): ä»¥ä¸‹ã®è©³ç´°ãªç™ºè¦‹äº‹é …ã‚’ç¢ºèª
2. **çŸ­æœŸ** (< 24æ™‚é–“): æ¨å¥¨ã•ã‚Œã‚‹ä¿®å¾©ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ
3. **é•·æœŸ** (< 1é€±é–“): æ”¹å–„ã®ãŸã‚ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç›£è¦–
4. **ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—**: è©²å½“ã™ã‚‹å ´åˆã¯ãƒã‚¹ãƒˆã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"""

    def format_plan_approval(self, plan: Dict[str, Any], query: str) -> str:
        """è¨ˆç”»æ‰¿èªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ãª Markdown ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™ã€‚"""
        output = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        output.append("# ğŸ“‹ èª¿æŸ»è¨ˆç”»")
        output.append("")
        output.append(f"**ã‚¯ã‚¨ãƒª:** {query}")
        output.append(f"**è¤‡é›‘åº¦:** {plan.get('complexity', 'unknown').title()}")
        output.append("")

        # è¨ˆç”»ã‚¹ãƒ†ãƒƒãƒ—
        steps = plan.get("steps", [])
        if steps:
            output.append("## èª¿æŸ»ã‚¹ãƒ†ãƒƒãƒ—")
            output.append("")
            for i, step in enumerate(steps, 1):
                output.append(f"{i}. {step}")
            output.append("")

        # è¨ˆç”»è©³ç´°
        reasoning = plan.get("reasoning", "æ¨™æº–çš„ãªèª¿æŸ»ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ")
        auto_execute = plan.get("auto_execute", False)

        output.append("## è¨ˆç”»è©³ç´°")
        output.append("")
        output.append(f"**ç†ç”±:** {reasoning}")
        output.append(f"**è‡ªå‹•å®Ÿè¡Œ:** {'ã¯ã„' if auto_execute else 'ã„ã„ãˆ'}")
        output.append("")

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        output.append("## åˆ©ç”¨å¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        output.append("")
        output.append("- `proceed` ã¾ãŸã¯ `yes` ã¨å…¥åŠ›ã—ã¦è¨ˆç”»ã‚’å®Ÿè¡Œ")
        output.append("- `modify` ã¨å…¥åŠ›ã—ã¦å¤‰æ›´ã‚’ææ¡ˆ")
        output.append("- ä»»æ„ã®ã‚¹ãƒ†ãƒƒãƒ—ã«ã¤ã„ã¦å…·ä½“çš„ãªè³ªå•ã‚’ã™ã‚‹")
        output.append("")

        return "\n".join(output)


def create_formatter(llm_provider: Optional[str] = None) -> SREOutputFormatter:
    """æ–°ã—ã„ SRE å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦è¿”ã—ã¾ã™ã€‚"""
    return SREOutputFormatter(llm_provider=llm_provider)
