{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Lab 5: AgentCore Evaluations - カスタマーサポートエージェントのオンライン評価\n",
    "\n",
    "### 概要\n",
    "\n",
    "このLabでは、AgentCore Evaluations を使用して、Lab 4 の本番カスタマーサポートエージェントを継続的に監視する方法を示します。顧客がエージェントとやり取りする際にリアルタイムでエージェントのパフォーマンスを自動的に評価するオンライン評価を設定します。\n",
    "\n",
    "**ワークショップの流れ:**\n",
    "\n",
    "- **Lab 1（完了）:** エージェントプロトタイプの作成 - 機能するカスタマーサポートエージェントを構築\n",
    "- **Lab 2（完了）:** Memory による強化 - 会話コンテキストとパーソナライゼーションを追加\n",
    "- **Lab 3（完了）:** Gateway と Identity によるスケーリング - エージェント間でツールを安全に共有\n",
    "- **Lab 4（完了）:** 本番環境へのデプロイ - AgentCore Runtime を使用してオブザーバビリティを実現\n",
    "- **Lab 5（現在）:** エージェントパフォーマンスの評価 - オンライン評価で品質を監視\n",
    "- **Lab 6:** ユーザーインターフェースの構築 - 顧客向けアプリケーションを作成\n",
    "\n",
    "### 学習内容\n",
    "\n",
    "組み込み評価器でオンライン評価を設定し、テストインタラクションを生成し、AgentCore Observability ダッシュボードを通じて品質メトリクスを分析して、エージェントのパフォーマンスを向上させます。\n",
    "\n",
    "### オンライン評価の概要\n",
    "\n",
    "オンライン評価は、選択した特定のインタラクションを分析するオンデマンド評価とは異なり、本番環境でデプロイされたエージェントを継続的に監視します。3つのコンポーネントで構成されます：設定可能なルールを持つセッションサンプリング、複数の評価方法（組み込みまたはカスタム評価器）、品質トレンドと低スコアセッション調査を備えたダッシュボードによる監視。\n",
    "\n",
    "エージェントは AgentCore Runtime で実行されているため、AgentCore Observability は [OTEL](https://opentelemetry.io/) インストルメンテーションを使用して自動的にコードをインストルメント化し、包括的なログとトレースを提供します。\n",
    "\n",
    "### 前提条件\n",
    "\n",
    "Lab 4 を完了して、カスタマーサポートエージェントがデプロイされている必要があります。Evaluations 権限を持つ Amazon Bedrock AgentCore への AWS アカウントアクセスが必要です。\n",
    "\n",
    "### アーキテクチャ\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/architecture_lab5_evaluation.png\" width=\"75%\"/>\n",
    "</div>\n",
    "\n",
    "*オンライン評価は、エージェントのインタラクションを自動的に監視し、サンプリングルールに基づいて評価器を適用し、結果を分析のために CloudWatch に出力します。*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### ステップ 1: 必要なライブラリのインポートとクライアントの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation, Runtime\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from boto3.session import Session\n",
    "from IPython.display import Markdown, display\n",
    "from lab_helpers.utils import get_ssm_parameter, get_or_create_cognito_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(f\"リージョン: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client = Evaluation(region=region)\n",
    "runtime_client = Runtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### ステップ 2: Lab 4 からのエージェント情報の取得\n",
    "\n",
    "Lab 4 のデプロイ中に SSM Parameter Store に保存されたカスタマーサポートエージェント ARN を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get agent ARN from SSM parameter store (saved in Lab 4)\n",
    "    agent_arn = get_ssm_parameter(\"/app/customersupport/agentcore/runtime_arn\")\n",
    "    \n",
    "    # Extract agent ID from ARN\n",
    "    agent_id = agent_arn.split(\":\")[-1].split(\"/\")[-1]\n",
    "    \n",
    "    # Set runtime client config path\n",
    "    runtime_client._config_path = Path.cwd() / \".bedrock_agentcore.yaml\"\n",
    "    \n",
    "    print(\"エージェント ID:\", agent_id)\n",
    "    print(\"エージェント ARN:\", agent_arn)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"\"\"Lab 4 からのエージェント情報がありません。最初に lab-04-agentcore-runtime.ipynb を実行してください。エラー: {str(e)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### ステップ 3: オンライン評価設定の作成\n",
    "\n",
    "カスタマーサポートエージェント用のオンライン評価設定を作成しましょう。組み込み評価器を使用して、エージェントパフォーマンスのさまざまな側面を評価します：\n",
    "\n",
    "- **Builtin.GoalSuccessRate** - エージェントがユーザーの目標をどの程度達成しているかを測定\n",
    "- **Builtin.Correctness** - レスポンスの事実の正確性を評価\n",
    "- **Builtin.ToolSelectionAccuracy** - 適切なツール選択を評価\n",
    "\n",
    "デモンストレーションのためにサンプリングレートを100%に設定していますが、本番環境ではトラフィック量に基づいてより低いレート（例：10〜20%）を使用する場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = eval_client.create_online_config(\n",
    "    agent_id=agent_id,\n",
    "    config_name=\"customer_support_agent_eval\",\n",
    "    sampling_rate=100,  # Evaluate 100% of sessions for demo\n",
    "    evaluator_list=[\n",
    "        \"Builtin.GoalSuccessRate\", \n",
    "        \"Builtin.Correctness\",\n",
    "        \"Builtin.ToolSelectionAccuracy\"\n",
    "    ],\n",
    "    config_description=\"Customer support agent online evaluation\",\n",
    "    auto_create_execution_role=True\n",
    ")\n",
    "\n",
    "print(\"オンライン評価設定が正常に作成されました！\")\n",
    "print(f\"設定 ID: {response['onlineEvaluationConfigId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### ステップ 4: 設定ステータスの確認\n",
    "\n",
    "詳細を取得して、評価設定が適切に作成され有効になっていることを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_details = eval_client.get_online_config(config_id=response['onlineEvaluationConfigId'])\n",
    "print(\"設定詳細:\")\n",
    "print(json.dumps(config_details, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### ステップ 5: テストインタラクションの生成\n",
    "\n",
    "さまざまなクエリでカスタマーサポートエージェントを呼び出して、評価用のトレースを生成します。異なるテストシナリオにより、評価器がエージェントのパフォーマンスをどのように評価するかを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get authentication token\n",
    "access_token = get_or_create_cognito_pool(refresh_token=True)\n",
    "print(f\"アクセストークンを取得しました: {access_token['bearer_token'][:20]}...\")\n",
    "\n",
    "def invoke_agent_runtime(prompt, session_id=None):\n",
    "    \"\"\"Invoke the agent runtime using starter toolkit\"\"\"\n",
    "    if not session_id:\n",
    "        session_id = str(uuid.uuid4())\n",
    "    \n",
    "    response = runtime_client.invoke(\n",
    "        payload={\"prompt\": prompt},\n",
    "        session_id=session_id,\n",
    "        bearer_token=access_token['bearer_token']\n",
    "    )\n",
    "    \n",
    "    return response, session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### テストシナリオ 1: 製品情報クエリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = str(uuid.uuid4())\n",
    "response, _ = invoke_agent_runtime(\n",
    "    \"I need information about the Gaming Console Pro. What are its specifications and price?\",\n",
    "    session1\n",
    ")\n",
    "print(\"顧客クエリ: 製品情報リクエスト\")\n",
    "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### テストシナリオ 2: テクニカルサポートリクエスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = str(uuid.uuid4())\n",
    "response, _ = invoke_agent_runtime(\n",
    "    \"My laptop won't start up. Can you help me troubleshoot this issue?\",\n",
    "    session2\n",
    ")\n",
    "print(\"顧客クエリ: テクニカルサポートリクエスト\")\n",
    "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### テストシナリオ 3: 返品ポリシーの問い合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "session3 = str(uuid.uuid4())\n",
    "response, _ = invoke_agent_runtime(\n",
    "    \"I bought a smartphone last week but it's not working properly. What's your return policy?\",\n",
    "    session3\n",
    ")\n",
    "print(\"顧客クエリ: 返品ポリシーの問い合わせ\")\n",
    "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### テストシナリオ 4: 複雑なマルチツールクエリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "session4 = str(uuid.uuid4())\n",
    "response, _ = invoke_agent_runtime(\n",
    "    \"I need help with my Gaming Console Pro. First, can you tell me about its warranty? Then I need technical support for connection issues.\",\n",
    "    session4\n",
    ")\n",
    "print(\"顧客クエリ: 複雑なマルチツールリクエスト\")\n",
    "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### テストシナリオ 5: 一般的な機能クエリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "session5 = str(uuid.uuid4())\n",
    "response, _ = invoke_agent_runtime(\n",
    "    \"What kind of support can you provide? List all your available tools and capabilities.\",\n",
    "    session5\n",
    ")\n",
    "print(\"顧客クエリ: 機能の問い合わせ\")\n",
    "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### ステップ 6: 評価結果の監視\n",
    "\n",
    "AgentCore Observability コンソールで評価結果を監視します。システムがトレースを処理し評価器を適用するため、結果が表示されるまで数分かかる場合があります。\n",
    "\n",
    "#### ダッシュボードへのアクセス\n",
    "\n",
    "1. [AgentCore Observability コンソール](https://console.aws.amazon.com/cloudwatch/home#gen-ai-observability/agent-core/agents) に移動します\n",
    "2. エージェントリストでカスタマーサポートエージェントを見つけます\n",
    "3. `DEFAULT` エンドポイントをクリックして評価メトリクスを表示します\n",
    "4. トレースとセッションビューで評価スコアを探します\n",
    "\n",
    "#### 表示される内容\n",
    "\n",
    "ダッシュボードには以下が表示されます：\n",
    "- **Goal Success Rate**: エージェントが顧客の目標をどの程度達成しているか\n",
    "- **Correctness**: 提供された情報の正確性\n",
    "- **Tool Selection Accuracy**: クエリに対する適切なツール選択\n",
    "\n",
    "![オンライン評価ダッシュボード](images/online_evaluations_dashboard.png)\n",
    "\n",
    "*AgentCore Observability ダッシュボードに表示される評価メトリクス*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### ステップ 7: 評価メトリクスの理解\n",
    "\n",
    "**Goal Success Rate** は、エージェントが顧客の主要な意図に正常に対応しているかどうかを測定します。高いスコアは効果的な問題解決を示し、低いスコアは満たされていないニーズ、不完全なレスポンス、または誤解されたリクエストを示唆します。\n",
    "\n",
    "**Correctness** は、レスポンスの事実の正確性を評価します。高いスコアは正確で信頼できる情報を示し、低いスコアは誤った事実、古い情報、または誤解を招くガイダンスを示唆します。\n",
    "\n",
    "**Tool Selection Accuracy** は、エージェントが各タスクに適切なツールを選択しているかどうかを評価します。高いスコアは適切なツール選択を示し、低いスコアは間違ったツール、不要な呼び出し、またはツール使用の欠落を示唆します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### ステップ 8: 結果の分析と次のステップ\n",
    "\n",
    "**Goal Success Rate が低い場合:** エージェントのシステムプロンプトを改善し、ツールの説明とパラメータを改善し、特定のトレーニング例を追加します。\n",
    "\n",
    "**Correctness スコアが低い場合:** Knowledge Base を最新の情報で更新し、ファクトチェックメカニズムを改善し、ツールのレスポンスを確認します。\n",
    "\n",
    "**ツール関連の問題の場合:** ツールパラメータスキーマを改善し、ツール選択ロジックを改善し、ツールドキュメントを強化します。\n",
    "\n",
    "**継続的な監視:** 評価メトリクス用の CloudWatch アラームを設定し、トレンド分析用のダッシュボードを作成し、品質低下の自動アラートを実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### ステップ 9: クリーンアップ（オプション）\n",
    "\n",
    "必要に応じて、以下のコードのコメントを解除してオンライン評価設定を無効にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if you want to disable the evaluation configuration\n",
    "# eval_client.delete_online_config(config_id=response['onlineEvaluationConfigId'])\n",
    "# print(\"オンライン評価設定が無効化されました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### おめでとうございます！\n",
    "\n",
    "**Lab 5: AgentCore Evaluations - オンライン評価**を正常に完了しました！\n",
    "\n",
    "### 達成したこと\n",
    "\n",
    "Goal Success Rate（顧客満足度と問題解決）、Correctness（事実の正確性）、Tool Selection Accuracy（適切なツール使用）を評価する組み込み評価器で、カスタマーサポートエージェントの自動継続的オンライン評価を設定しました。評価結果は、リアルタイムのインサイトのために AgentCore Observability ダッシュボードと統合されています。\n",
    "\n",
    "**主な利点:** 顧客への影響前に問題をキャッチするプロアクティブな品質保証、改善をガイドするデータドリブンな最適化、大規模なパフォーマンス監視による本番環境の信頼性、パターンと機会を特定する継続的な学習。\n",
    "\n",
    "**次のステップ:** 評価ダッシュボードを定期的に監視し、品質しきい値用の CloudWatch アラームを設定し、インサイトを使用してエージェントを反復的に改善し、ドメイン固有のメトリクス用のカスタム評価器の追加を検討してください。\n",
    "\n",
    "### 次のステップ: [Lab 6: ユーザーインターフェースの構築 →](lab-06-frontend.ipynb)\n",
    "\n",
    "品質監視されたエージェントと顧客がやり取りするための、ユーザーフレンドリーなウェブインターフェースを構築して、顧客体験を完成させましょう。\n",
    "\n",
    "カスタマーサポートエージェントは、包括的な品質監視を備えて本番環境対応になりました！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
