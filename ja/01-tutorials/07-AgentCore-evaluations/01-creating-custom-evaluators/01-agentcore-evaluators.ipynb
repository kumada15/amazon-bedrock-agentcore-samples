{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## AgentCore Evaluations - 評価メトリクスの作成\n",
    "\n",
    "このチュートリアルでは、AgentCore Evaluations のビルトインおよびカスタムメトリクスについて学びます。\n",
    "それぞれのタイプをいつ使用するか、また特定のニーズに合わせたカスタム評価メトリクスの作成方法を学びます。\n",
    "\n",
    "### 学習内容\n",
    "- ビルトイン評価メトリクスとそのユースケースの理解\n",
    "- 特殊な要件に対応するカスタム評価メトリクスの作成\n",
    "- エージェントに適した評価アプローチの選択\n",
    "\n",
    "### チュートリアル詳細\n",
    "\n",
    "| 情報               | 詳細                                                                           |\n",
    "|:-------------------|:-------------------------------------------------------------------------------|\n",
    "| チュートリアルタイプ | カスタム評価メトリクスの作成                                                    |\n",
    "| LLM モデル          | Anthropic Claude Haiku 4.5                                                     |\n",
    "| チュートリアル構成   | ビルトイン評価メトリクスの一覧表示、カスタム評価メトリクスの作成                  |\n",
    "| チュートリアル分野   | 横断的                                                                         |\n",
    "| 複雑さ              | 簡単                                                                           |\n",
    "| 使用 SDK            | Amazon Bedrock AgentCore Starter toolkit                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 評価メトリクスの種類\n",
    "\n",
    "### ビルトイン評価メトリクス\n",
    "\n",
    "AgentCore は、エージェントのパフォーマンスを評価するために大規模言語モデル（LLM）をジャッジとして使用する13の事前設定された評価メトリクスを提供します。これらの評価メトリクスには、慎重に作成されたプロンプトテンプレートと事前選択された評価モデルが付属しており、異なるユースケース間で評価基準を標準化しています。すぐに使用でき、開始するために追加の設定は必要ありません。\n",
    "\n",
    "これらの評価メトリクスは4つの異なるグループに分けられます:\n",
    "\n",
    "- **応答品質**: エージェントが各ターンで期待通りに動作しているかを判断するのに役立つ評価メトリクス。トレースベースで動作し、各ユーザーとエージェントのインタラクションを評価します。\n",
    "- **タスク完了**: このカテゴリには、セッション全体を評価する1つの評価メトリクス（Goal success）があります。マルチターンの会話では、この評価メトリクスがユーザーの目標が達成されたか、最終的な結果が得られたかを判断するのに役立ちます。エージェントがフォローアップの質問をする状況では、この評価メトリクスは要求されたタスクが実際に完了したかを理解するために不可欠です。\n",
    "- **ツールレベル**: ツール呼び出しがどれだけ成功しているかを理解するのに役立つ評価メトリクス。ツールレベルでエージェントのツールとパラメータ選択の精度を測定します。エージェントが1回のターンで2つ以上の異なるツールを呼び出す場合、それぞれがトレース内で独自のメトリクスを持ちます。\n",
    "- **安全性**: エージェントが有害であるか、個人やグループについてステレオタイプ的な一般化を行っているかを検出する評価メトリクス。\n",
    "\n",
    "ビルトインメトリクスを使用する場合、プロンプトからモデルまですべてが処理されます。つまり、評価メトリクスを変更することはできません。これは、すべてのユーザー間で一貫した信頼性のある評価を維持するためです。ただし、ビルトインメトリクスを基に独自のメトリクスを作成することができます。そのために、ビルトイン評価メトリクス用の**プロンプトテンプレート**を提供しています。\n",
    "\n",
    "### カスタム評価メトリクス\n",
    "\n",
    "カスタム評価メトリクスは、LLM を基盤となるジャッジとして活用しながら、評価プロセスのあらゆる側面を定義できる最大限の柔軟性を提供します。カスタム評価メトリクスでは以下をカスタマイズできます:\n",
    "\n",
    "- **評価モデル**: 評価ニーズに最適な LLM を選択\n",
    "- **評価プロンプト**: ユースケースに特化した評価指示を作成\n",
    "- **スコアリングスキーマ**: 組織のメトリクスに合わせたスコアリングシステムを設計\n",
    "\n",
    "### AgentCore Observability でエージェントからトレースを生成\n",
    "\n",
    "AgentCore Observability は、[OpenTelemetry (OTEL)](https://opentelemetry.io/) トレースを活用して詳細な実行データをキャプチャし構造化することで、呼び出し中のエージェントの動作を包括的に可視化します。AgentCore は [AWS Distro for OpenTelemetry (ADOT)](https://aws-otel.github.io/) を利用して、さまざまなエージェントフレームワーク間で異なるタイプの OTEL トレースを計装します。\n",
    "\n",
    "エージェントが AgentCore Runtime でホストされている場合（このチュートリアルのエージェントのように）、AgentCore Observability の計装は最小限の設定で自動的に行われます。必要なのは `requirements.txt` に `aws-opentelemetry-distro` を含めることだけで、AgentCore Runtime が OTEL 設定を自動的に処理します。エージェントが AgentCore Runtime で実行されていない場合は、AgentCore Observability で利用できるようにするために ADOT で計装する必要があります。テレメトリデータを CloudWatch に送信するための環境変数を設定し、OpenTelemetry 計装でエージェントを実行する必要があります。\n",
    "\n",
    "プロセスは以下のようになります:\n",
    "\n",
    "![session_traces](../images/observability_traces.png)\n",
    "\n",
    "セッショントレースが AgentCore Observability で利用可能になると、AgentCore Evaluations を使用してエージェントの動作を評価できます。\n",
    "\n",
    "### 評価レベル\n",
    "AgentCore Evaluations は、エージェントインタラクションの異なるレベルで動作します。セッション情報を使用して、会話全体のやり取りを分析できます。また、トレース情報を使用して、会話の個別のターンでのユーザーの質問に対するエージェントの応答を評価することもできます。さらに、スパンデータを使用して、ツール呼び出しやパラメータ選択を含むターン内の情報を評価することもできます。\n",
    "\n",
    "異なるレベルに対してカスタムメトリクスを作成できます。ビルトインメトリクスは以下のスコープで動作します:\n",
    "\n",
    "![metrics level](../images/metrics_per_level.png)\n",
    "\n",
    "このチュートリアルでは、トレースレベルでメトリクスを作成します。\n",
    "\n",
    "### チュートリアルの成果\n",
    "\n",
    "このチュートリアルを終えると、AgentCore Evaluation のビルトインおよびカスタムメトリクスについて学んだことになります。また、エージェントの応答品質を測定するためのカスタムメトリクスを作成したことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### 前提条件\n",
    "このチュートリアルを実行するには以下が必要です:\n",
    "* Python 3.10以上\n",
    "* AWS 認証情報\n",
    "* Amazon Bedrock AgentCore Starter toolkit\n",
    "\n",
    "### AgentCore Evaluations の使用\n",
    "\n",
    "Amazon Bedrock AgentCore は、エージェントとツールの開発、デプロイ、監視のためのさまざまなインターフェースをサポートしています。\n",
    "\n",
    "完全な制御が必要な場合は、[コントロールプレーン](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/Welcome.html)と[データプレーン](https://docs.aws.amazon.com/bedrock-agentcore/latest/APIReference/Welcome.html) API を使用できます。これらの API は AWS SDK ([boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html)、[AWS SDK for Java](https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/home.html)、[AWS SDK for JavaScript](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/welcome.html)) やその他の AWS 開発者ツール（AWS Command Line Interface ([AWS CLI](https://aws.amazon.com/cli/)) など）を通じて公開されています。\n",
    "\n",
    "簡素化されたアプローチとして、[AgentCore Python SDK](https://github.com/aws/bedrock-agentcore-sdk-python) と [AgentCore Starter Toolkit](https://github.com/aws/bedrock-agentcore-starter-toolkit) を使用することもできます。AgentCore Python SDK はエージェント開発用の Python プリミティブを提供し、AgentCore Starter Toolkit は CLI ツールと AgentCore 機能のより高レベルの抽象化を提供します。\n",
    "\n",
    "![agentcore_interfaces](../images/agentcore_interfaces.png)\n",
    "\n",
    "このチュートリアルでは、簡素化された体験のために AgentCore Starter Toolkit を使用します。`03-advanced` フォルダには、boto3 を直接使用する例があります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation\n",
    "import os\n",
    "import json\n",
    "from boto3.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### AgentCore 評価クライアントの初期化\n",
    "\n",
    "それでは評価クライアントを初期化しましょう。このチュートリアルでは、[AgentCore Starter Toolkit](https://github.com/aws/bedrock-agentcore-starter-toolkit) を使用します。これは AgentCore コンポーネントとのやり取りを簡素化し、開始プロセスを加速する抽象化 SDK です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client = Evaluation(region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### ビルトイン評価メトリクスの取得\n",
    "\n",
    "利用可能なビルトイン評価メトリクスを取得して、どこで使用できるかを理解しましょう。`list_evaluators()` 関数がそれを助けてくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_evaluators = eval_client.list_evaluators()\n",
    "available_evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "情報を辞書として取得し、後でオンデマンドおよびオンライン評価で使用することもできます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    available_evaluators['evaluators'][0]['evaluatorId'], \n",
    "    available_evaluators['evaluators'][0]['description']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "ご覧の通り、`Builtin.Correctness` メトリクスは応答品質の評価に役立ちます。このメトリクスの詳細を理解するために深掘りしましょう。そのために `get_evaluator` メソッドを使用できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client.get_evaluator(evaluator_id=\"Builtin.Correctness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "この場合、評価メトリクスは応答を3つのレベル（Incorrect、Partially Correct、Correct）に分類していることがわかります。私たちのユースケースでは、もう少し詳細に5段階のスケールを使用したいと考えています。これを実現するために、カスタム評価メトリクスを作成できます。\n",
    "\n",
    "### カスタム評価メトリクスの作成\n",
    "\n",
    "5段階スケールを持つ応答品質のカスタムメトリクスを作成しましょう。そのためには、評価モデルを選択し、評価の指示を提供し、評価スケールを設定する必要があります。この場合、スケールは Very Good から Very Poor までとなります。JSON ファイルから評価設定を取得しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": "with open(\"metric.json\") as f:\n    print(\"カスタムメトリクスの詳細を読み込み中\")\n    eval_config = json.load(f)\neval_config"
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "次に、`create_evaluator` メソッドを使用して評価メトリクスを作成できます。`TRACE` レベルで `response_Quality` 評価メトリクスを作成します。\n",
    "\n",
    "カスタム評価メトリクスを作成する際、ツール呼び出しレベル、トレースレベル、またはセッションレベルで適用することを選択できます。\n",
    "\n",
    "* **ツール呼び出し**は、エージェントが外部関数、API、または機能を呼び出すことを表すスパンです。ツール呼び出しスパンは通常、ツール名、入力パラメータ、実行時間、出力などの情報をキャプチャします。ツール呼び出しの詳細は、エージェントがツールを正しく効率的に選択して使用したかどうかを評価するために使用されます。\n",
    "\n",
    "* **トレース**は、単一のエージェント実行またはリクエストの完全な記録です。トレースには1つ以上のスパンが含まれ、その実行中に行われた個々の操作を表します。トレースはエージェントの決定とツール使用に対するエンドツーエンドの可視性を提供します。\n",
    "\n",
    "* **セッション**は、単一のユーザーまたはワークフローからの関連するインタラクションの論理的なグループを表します。セッションには1つ以上のトレースが含まれる場合があります。セッションは、個々のリクエストに焦点を当てるのではなく、マルチステップのインタラクション全体でエージェントの動作を表示および評価するのに役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_evaluator = eval_client.create_evaluator(\n",
    "    name=\"response_quality_for_scope\",\n",
    "    level=\"TRACE\",\n",
    "    description=\"Response quality evaluator\",\n",
    "    config=eval_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 次のチュートリアル用に評価メトリクス情報を保存\n",
    "\n",
    "次のチュートリアルで使用するために evaluatorId を保存します。`evaluator_id` 変数を保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_id = custom_evaluator['evaluatorId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store evaluator_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### おめでとうございます\n",
    "\n",
    "次のチュートリアルで使用するカスタム評価メトリクスを作成しました。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}