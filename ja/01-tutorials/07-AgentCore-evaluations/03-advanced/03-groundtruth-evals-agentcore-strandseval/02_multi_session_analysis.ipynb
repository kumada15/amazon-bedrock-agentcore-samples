{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マルチセッション評価\n",
    "\n",
    "このノートブックは、LLM をジャッジとして使用する拡張可能な LLM ベース評価フレームワークである Strands Evals を使用してエージェントセッションを評価します。各セッションについて、AgentCore Observability からトレースを取得し、評価メトリクスを実行し、ダッシュボード相関のために元のトレース ID で結果をログに記録します。\n",
    "\n",
    "**このノートブックは2つの評価メトリクスをデモンストレーションします:**\n",
    "- **OutputEvaluator**: 応答品質をスコアリング（関連性、正確性、完全性）\n",
    "- **TrajectoryEvaluator**: ツール使用をスコアリング（選択、効率、シーケンス）\n",
    "\n",
    "Strands Evals は、ほぼあらゆる評価タイプのカスタム評価メトリクスをサポートしています。フレームワークの力はルーブリックシステムにあります。基準を定義すれば、LLM がそれらを一貫して適用します。\n",
    "\n",
    "**ワークフロー:**\n",
    "1. 検出ノートブックからセッションを読み込み（またはカスタムセッション ID を提供）\n",
    "2. 各セッション: トレースを取得、評価ケースを作成、評価メトリクスを実行\n",
    "3. EMF 形式で結果を AgentCore にログ\n",
    "4. サマリー統計を生成\n",
    "\n",
    "**前提条件:** まずセッション検出ノートブックを実行するか、セッション ID のリストを準備してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この位置づけ\n",
    "\n",
    "これは**ノートブック2（オプションA）**です - 定義したカスタムルーブリックを使用してセッションを評価します。\n",
    "\n",
    "![Notebook Workflow](images/notebook_workflow.svg)\n",
    "\n",
    "## データフロー\n",
    "\n",
    "評価パイプラインは AgentCore Observability トレースをスコア付き結果に変換します:\n",
    "\n",
    "![Evaluation Pipeline](images/evaluation_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ\n",
    "\n",
    "Strands Evals 評価メトリクスと AgentCore Observability インタラクション用のユーティリティクラスを含む必要なモジュールをインポートします。設定は `config.py` から読み込まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from config import (\n",
    "    AWS_REGION,\n",
    "    AWS_ACCOUNT_ID,\n",
    "    SOURCE_LOG_GROUP,\n",
    "    EVAL_RESULTS_LOG_GROUP,\n",
    "    LOOKBACK_HOURS,\n",
    "    MAX_CASES_PER_SESSION,\n",
    "    DISCOVERED_SESSIONS_PATH,\n",
    "    RESULTS_JSON_PATH,\n",
    "    EVALUATION_CONFIG_ID,\n",
    "    setup_cloudwatch_environment,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    CloudWatchSessionMapper,\n",
    "    ObservabilityClient,\n",
    "    SessionDiscoveryResult,\n",
    "    SessionInfo,\n",
    "    send_evaluation_to_cloudwatch,\n",
    ")\n",
    "\n",
    "from strands_evals import Case, Experiment\n",
    "from strands_evals.evaluators import OutputEvaluator, TrajectoryEvaluator\n",
    "from strands_evals.types.trace import AgentInvocationSpan, ToolExecutionSpan\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定\n",
    "\n",
    "CloudWatch メトリクス用の評価メトリクス名を定義します。これらの名前は AgentCore Observability ダッシュボードに表示され、`Custom.YourEvaluatorName` 規約に従う必要があります。`EVALUATION_CONFIG_ID` は `config.py` から読み込まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator names for CloudWatch metrics (customize for your use case)\n",
    "OUTPUT_EVALUATOR_NAME = \"Custom.OutputEvaluator\"\n",
    "TRAJECTORY_EVALUATOR_NAME = \"Custom.TrajectoryEvaluator\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudWatch 環境\n",
    "\n",
    "評価結果のログに必要な環境変数を設定します。OTEL リソース属性に `config.py` の `SERVICE_NAME` を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_cloudwatch_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セッションの読み込み\n",
    "\n",
    "検出ノートブックの JSON 出力からセッションを読み込みます。または、`USE_JSON_FILE = False` に設定し、特定のセッションのターゲット再評価のためにカスタムセッション ID を直接提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to False to provide custom session IDs instead\n",
    "USE_JSON_FILE = True\n",
    "\n",
    "if USE_JSON_FILE:\n",
    "    discovery_result = SessionDiscoveryResult.load_from_json(DISCOVERED_SESSIONS_PATH)\n",
    "    sessions_to_process = discovery_result.sessions\n",
    "else:\n",
    "    # Provide custom session IDs here\n",
    "    session_ids = [\n",
    "        \"your-session-id-here\",\n",
    "    ]\n",
    "    sessions_to_process = [\n",
    "        SessionInfo(\n",
    "            session_id=sid,\n",
    "            span_count=0,\n",
    "            first_seen=datetime.now(timezone.utc),\n",
    "            last_seen=datetime.now(timezone.utc),\n",
    "            discovery_method=\"user_provided\",\n",
    "        )\n",
    "        for sid in session_ids\n",
    "    ]\n",
    "\n",
    "print(f\"Loaded {len(sessions_to_process)} sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価メトリクスのルーブリック\n",
    "\n",
    "ルーブリックは評価基準を定義します。評価メトリクスはルーブリックとエージェントの出力を LLM に送信し、LLM がジャッジとして機能してスコア（0.0-1.0）と説明を返します。\n",
    "\n",
    "**効果的なルーブリックの作成:**\n",
    "- 良い品質と悪い品質を構成するものについて具体的に記述する\n",
    "- スコアリングアンカーを含める（1.0 vs 0.5 vs 0.0 は何を意味するか？）\n",
    "- エージェントのドメインに関連する測定可能な基準に焦点を当てる\n",
    "\n",
    "以下のルーブリックをカスタマイズしてください。デフォルトのルーブリックは一般的な応答品質とツール使用パターンを評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rubric = \"\"\"\n",
    "Evaluate the agent's response based on:\n",
    "1. Relevance: Does the response directly address the user's question?\n",
    "2. Accuracy: Is the information factually correct?\n",
    "3. Completeness: Does the response provide sufficient detail?\n",
    "\n",
    "Score 0.0-1.0: 1.0=excellent, 0.5=adequate, 0.0=poor\n",
    "\"\"\"\n",
    "\n",
    "trajectory_rubric = \"\"\"\n",
    "Evaluate the agent's tool usage based on:\n",
    "1. Tool Selection: Did the agent choose appropriate tools?\n",
    "2. Efficiency: Were tools used without unnecessary calls?\n",
    "3. Logical Sequence: Were tools used in a logical order?\n",
    "\n",
    "Score 0.0-1.0: 1.0=optimal, 0.5=acceptable, 0.0=poor\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ヘルパー関数\n",
    "\n",
    "これらの関数は AgentCore Observability トレースと Strands Evals を橋渡しします:\n",
    "\n",
    "- `task_fn(case)`: OutputEvaluator がルーブリックに対してスコアリングするためのエージェントの実際の応答を返します。\n",
    "\n",
    "- `trajectory_task_fn(case)`: TrajectoryEvaluator がツール使用パターンを評価するための応答とツールシーケンスの両方を返します。\n",
    "\n",
    "- `create_cases_from_session(session)`: Strands Eval Session を評価 Cases に変換します。AgentInvocationSpan からユーザープロンプトを抽出し、ToolExecutionSpan オブジェクトからツール名を抽出し、CloudWatch 相関用に元の trace_id を保持します。\n",
    "\n",
    "- `log_case_result_to_cloudwatch(case, ...)`: 元の trace_id を使用して評価結果を AgentCore Observability に送信し、ダッシュボードで元のトレースと一緒にスコアを確認できるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_fn(case: Case) -> str:\n",
    "    \"\"\"Return actual output from trace metadata.\"\"\"\n",
    "    return (case.metadata.get(\"actual_output\", \"\"))\n",
    "\n",
    "\n",
    "def trajectory_task_fn(case: Case):\n",
    "    \"\"\"Return output and trajectory from trace metadata.\"\"\"\n",
    "    return {\"output\": case.metadata.get(\"actual_output\", \"\"), \"trajectory\": case.metadata.get(\"trajectory_for_eval\", [])}\n",
    "\n",
    "def log_case_result_to_cloudwatch(case: Case, evaluator_name: str, score: float, explanation: str, label: str = None) -> bool:\n",
    "    \"\"\"Log evaluation result to CloudWatch with original trace ID.\"\"\"\n",
    "    trace_id = case.metadata.get(\"trace_id\", \"\")\n",
    "    if not trace_id:\n",
    "        return False\n",
    "    return send_evaluation_to_cloudwatch(\n",
    "        trace_id=trace_id,\n",
    "        session_id=case.session_id,\n",
    "        evaluator_name=evaluator_name,\n",
    "        score=score,\n",
    "        explanation=explanation,\n",
    "        label=label,\n",
    "        config_id=EVALUATION_CONFIG_ID,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_cases_from_session(session, session_id: str, max_cases: int = None) -> List[Case]:\n",
    "    \"\"\"Create evaluation cases from a Strands Eval Session.\"\"\"\n",
    "    cases = []\n",
    "    for i, trace in enumerate(session.traces):\n",
    "        if max_cases and len(cases) >= max_cases:\n",
    "            break\n",
    "        agent_span = None\n",
    "        tool_names = []\n",
    "        for span in trace.spans:\n",
    "            if isinstance(span, AgentInvocationSpan):\n",
    "                agent_span = span\n",
    "            elif isinstance(span, ToolExecutionSpan):\n",
    "                tool_names.append(span.tool_call.name)\n",
    "        if agent_span:\n",
    "            case = Case(\n",
    "                name=f\"trace_{i+1}_{trace.trace_id[:8]}\",\n",
    "                input=agent_span.user_prompt or \"\",\n",
    "                expected_output=\"\",\n",
    "                session_id=session_id,\n",
    "                metadata={\n",
    "                    \"actual_output\": agent_span.agent_response or \"\",\n",
    "                    \"actual_trajectory\": tool_names,\n",
    "                    \"trace_id\": trace.trace_id,\n",
    "                    \"tool_count\": len(tool_names),\n",
    "                },\n",
    "            )\n",
    "            cases.append(case)\n",
    "    return cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クライアントの初期化\n",
    "\n",
    "トレースを取得するための `ObservabilityClient` と変換するための `CloudWatchSessionMapper` を作成します。\n",
    "\n",
    "マッパーは生の AgentCore Observability スパンを構造化された Strands Eval オブジェクトに変換します:\n",
    "- trace_id でスパンをグループ化して各インタラクションを再構築\n",
    "- ツール呼び出しを抽出し、結果と照合\n",
    "- ユーザープロンプト（最初のメッセージ）とエージェント応答（最終出力）を識別\n",
    "- AgentInvocationSpan（完全なインタラクション）と ToolExecutionSpan（各ツール使用）を生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_client = ObservabilityClient(\n",
    "    region_name=AWS_REGION,\n",
    "    log_group=SOURCE_LOG_GROUP,\n",
    ")\n",
    "mapper = CloudWatchSessionMapper()\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(hours=LOOKBACK_HOURS)\n",
    "start_time_ms = int(start_time.timestamp() * 1000)\n",
    "end_time_ms = int(end_time.timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セッションの処理\n",
    "\n",
    "メイン評価ループ。各セッションについて:\n",
    "1. AgentCore Observability からスパンを取得\n",
    "2. マッパーを使用してスパンを Strands Eval Session 形式に変換\n",
    "3. セッション内の各トレースから評価 Cases を作成\n",
    "4. すべてのケースに対して OutputEvaluator を実行\n",
    "5. ツールを使用したケースに対して TrajectoryEvaluator を実行\n",
    "6. ダッシュボード相関用に元のトレース ID でresすべての結果を AgentCore Observability にログ\n",
    "\n",
    "各セッションの進捗状況が表示されます。エラーはキャッチされてログに記録されますが、ループは停止しません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_session_results = []\n",
    "total_cases_evaluated = 0\n",
    "total_logs_sent = 0\n",
    "all_tools_used = set()\n",
    "\n",
    "for session_idx, session_info in enumerate(sessions_to_process):\n",
    "    session_id = session_info.session_id\n",
    "    print(f\"[{session_idx + 1}/{len(sessions_to_process)}] {session_id}\")\n",
    "\n",
    "    try:\n",
    "        trace_data = obs_client.get_session_data(\n",
    "            session_id=session_id,\n",
    "            start_time_ms=start_time_ms,\n",
    "            end_time_ms=end_time_ms,\n",
    "            include_runtime_logs=False,\n",
    "        )\n",
    "\n",
    "        if not trace_data.spans:\n",
    "            all_session_results.append({\"session_id\": session_id, \"status\": \"skipped\", \"reason\": \"no_spans\"})\n",
    "            continue\n",
    "\n",
    "        session = trace_data.to_session(mapper)\n",
    "        cases = create_cases_from_session(session, session_id, MAX_CASES_PER_SESSION)\n",
    "\n",
    "        if not cases:\n",
    "            all_session_results.append({\"session_id\": session_id, \"status\": \"skipped\", \"reason\": \"no_cases\"})\n",
    "            continue\n",
    "\n",
    "        for case in cases:\n",
    "            for tool in case.metadata.get(\"actual_trajectory\", []):\n",
    "                all_tools_used.add(tool)\n",
    "\n",
    "        # Run Output Evaluator\n",
    "        output_experiment = Experiment(cases=cases, evaluators=[OutputEvaluator(rubric=output_rubric)])\n",
    "        output_results = output_experiment.run_evaluations(task_fn)\n",
    "        output_report = output_results[0]\n",
    "\n",
    "        output_logged = 0\n",
    "        for i, case in enumerate(cases):\n",
    "            if log_case_result_to_cloudwatch(case, OUTPUT_EVALUATOR_NAME, output_report.scores[i], output_report.reasons[i] if i < len(output_report.reasons) else \"\"):\n",
    "                output_logged += 1\n",
    "\n",
    "        # Run Trajectory Evaluator\n",
    "        trajectory_cases = [c for c in cases if c.metadata.get(\"actual_trajectory\")]\n",
    "        trajectory_score = None\n",
    "        trajectory_logged = 0\n",
    "\n",
    "        if trajectory_cases:\n",
    "            traj_eval_cases = [\n",
    "                Case(name=c.name, input=c.input, expected_output=c.expected_output, session_id=c.session_id,\n",
    "                     metadata={**c.metadata, \"trajectory_for_eval\": c.metadata.get(\"actual_trajectory\", [])})\n",
    "                for c in trajectory_cases\n",
    "            ]\n",
    "            trajectory_experiment = Experiment(\n",
    "                cases=traj_eval_cases,\n",
    "                evaluators=[TrajectoryEvaluator(rubric=trajectory_rubric, trajectory_description={\"available_tools\": list(all_tools_used)})]\n",
    "            )\n",
    "            trajectory_results = trajectory_experiment.run_evaluations(trajectory_task_fn)\n",
    "            trajectory_report = trajectory_results[0]\n",
    "            trajectory_score = trajectory_report.overall_score\n",
    "\n",
    "            for i, case in enumerate(traj_eval_cases):\n",
    "                if log_case_result_to_cloudwatch(case, TRAJECTORY_EVALUATOR_NAME, trajectory_report.scores[i], trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"\"):\n",
    "                    trajectory_logged += 1\n",
    "\n",
    "        all_session_results.append({\n",
    "            \"session_id\": session_id,\n",
    "            \"status\": \"completed\",\n",
    "            \"case_count\": len(cases),\n",
    "            \"output_score\": output_report.overall_score,\n",
    "            \"trajectory_score\": trajectory_score,\n",
    "            \"logs_sent\": output_logged + trajectory_logged,\n",
    "        })\n",
    "        total_cases_evaluated += len(cases)\n",
    "        total_logs_sent += output_logged + trajectory_logged\n",
    "\n",
    "    except Exception as e:\n",
    "        all_session_results.append({\"session_id\": session_id, \"status\": \"error\", \"error\": str(e)})\n",
    "\n",
    "print(f\"\\nCompleted: {len([r for r in all_session_results if r['status'] == 'completed'])} sessions, {total_cases_evaluated} cases, {total_logs_sent} logs sent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サマリー\n",
    "\n",
    "完了率、評価されたケースの総数、出力およびトラジェクトリ評価メトリクスの両方の平均スコアを含む、すべての評価済みセッションの集計統計。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = [r for r in all_session_results if r.get(\"status\") == \"completed\"]\n",
    "output_scores = [r[\"output_score\"] for r in completed if r.get(\"output_score\") is not None]\n",
    "trajectory_scores = [r[\"trajectory_score\"] for r in completed if r.get(\"trajectory_score\") is not None]\n",
    "\n",
    "print(f\"Sessions: {len(completed)}/{len(all_session_results)} completed\")\n",
    "print(f\"Cases evaluated: {total_cases_evaluated}\")\n",
    "print(f\"CloudWatch logs sent: {total_logs_sent}\")\n",
    "\n",
    "if output_scores:\n",
    "    print(f\"Output score: avg={sum(output_scores)/len(output_scores):.2f}, min={min(output_scores):.2f}, max={max(output_scores):.2f}\")\n",
    "if trajectory_scores:\n",
    "    print(f\"Trajectory score: avg={sum(trajectory_scores)/len(trajectory_scores):.2f}, min={min(trajectory_scores):.2f}, max={max(trajectory_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セッションごとの結果\n",
    "\n",
    "出力およびトラジェクトリスコアを表示する各セッションの個別結果。「skipped」とマークされたセッションにはスパンまたは有効なケースがありませんでした。「error」とマークされたセッションは処理中に例外が発生しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in enumerate(all_session_results):\n",
    "    status = r.get(\"status\", \"unknown\")\n",
    "    if status == \"completed\":\n",
    "        print(f\"{i+1}. {r['session_id'][:20]}... output={r.get('output_score', 0):.2f} traj={r.get('trajectory_score') or '-'}\")\n",
    "    else:\n",
    "        print(f\"{i+1}. {r['session_id'][:20]}... {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果のエクスポート\n",
    "\n",
    "さらなる分析またはレポート用に評価結果を JSON に保存します。エクスポートには設定、サマリー統計、セッションごとの結果が含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "export_data = {\n",
    "    \"evaluation_time\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"config\": {\n",
    "        \"source_log_group\": SOURCE_LOG_GROUP,\n",
    "        \"eval_results_log_group\": EVAL_RESULTS_LOG_GROUP,\n",
    "        \"output_evaluator\": OUTPUT_EVALUATOR_NAME,\n",
    "        \"trajectory_evaluator\": TRAJECTORY_EVALUATOR_NAME,\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total_sessions\": len(all_session_results),\n",
    "        \"completed_sessions\": len(completed),\n",
    "        \"total_cases\": total_cases_evaluated,\n",
    "        \"avg_output_score\": sum(output_scores) / len(output_scores) if output_scores else None,\n",
    "        \"avg_trajectory_score\": sum(trajectory_scores) / len(trajectory_scores) if trajectory_scores else None,\n",
    "    },\n",
    "    \"session_results\": all_session_results,\n",
    "}\n",
    "\n",
    "with open(RESULTS_JSON_PATH, \"w\") as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"Exported to {RESULTS_JSON_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
