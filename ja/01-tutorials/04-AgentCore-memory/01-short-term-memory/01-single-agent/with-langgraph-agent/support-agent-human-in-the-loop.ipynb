{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LangGraph と AgentCore Memory - Human in the Loop（短期メモリ）\n",
    "\n",
    "## はじめに\n",
    "このノートブックでは、Amazon Bedrock AgentCore Memory 機能を LangGraph と統合して **Human-in-the-Loop** ワークフローを作成する方法を紹介します。**短期メモリ**の永続化と、人間の介入のためにエージェント実行を中断する機能を組み合わせ、シームレスなハンドオフを伴う高度なカスタマーサポートシナリオを実現します。\n",
    "\n",
    "## チュートリアルの詳細\n",
    "\n",
    "| 情報               | 詳細                                                                            |\n",
    "|:-------------------|:--------------------------------------------------------------------------------|\n",
    "| チュートリアルタイプ | 短期会話メモリ                                                                  |\n",
    "| エージェントのユースケース | 人間へのエスカレーション付きカスタマーサポート                                    |\n",
    "| エージェントフレームワーク | LangGraph                                                                       |\n",
    "| LLM モデル          | Anthropic Claude Haiku 4.5                                                     |\n",
    "| チュートリアルコンポーネント | AgentCore 短期メモリ、LangGraph Checkpointer、Human-in-the-Loop                  |\n",
    "| 難易度              | 初級                                                                            |\n",
    "\n",
    "学習内容：\n",
    "- ワークフロー永続化のための AgentCore Memory を使用したメモリチェックポインターの作成\n",
    "- Human-in-the-Loop ワークフロー用の LangGraph 中断メカニズムの使用\n",
    "- 人間の介入のために実行を一時停止できるツールの実装\n",
    "- LangGraph Command を使用した人間の入力後のエージェントワークフローの再開\n",
    "- シームレスなハンドオフを伴う複雑なカスタマーサポートシナリオの管理\n",
    "\n",
    "### シナリオのコンテキスト\n",
    "\n",
    "この例では、複雑な問題を人間のスーパーバイザーにエスカレーションできる「**カスタマーサポートエージェント**」を作成します。人間の専門知識が必要な状況に遭遇すると、エージェントは実行を一時停止し、現在の状態を AgentCore Memory に保存して、人間の介入を待ちます。その後、人間のスーパーバイザーがガイダンスを提供でき、エージェントは強化されたコンテキストで再開します。\n",
    "\n",
    "## アーキテクチャ\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/architecture.png\" width=\"65%\" />\n",
    "</div>\n",
    "\n",
    "## 前提条件\n",
    "\n",
    "- Python 3.10以上\n",
    "- 適切な権限を持つ AWS アカウント\n",
    "- AgentCore Memory の適切な権限を持つ AWS IAM ロール\n",
    "- Amazon Bedrock モデルへのアクセス\n",
    "\n",
    "### 統合の仕組み\n",
    "\n",
    "Human-in-the-Loop ワークフロー用の LangGraph と AgentCore Memory の統合には以下が含まれます：\n",
    "\n",
    "1. 永続的な状態管理のためのチェックポインターバックエンドとして AgentCore Memory を使用\n",
    "2. 特定のポイントで実行を一時停止する中断メカニズムの実装\n",
    "3. 人間のスーパーバイザーが追加のコンテキストでワークフローを再開できるようにする\n",
    "4. 中断を超えて会話履歴と状態を維持\n",
    "\n",
    "このアプローチにより、AI エージェントと人間のスーパーバイザーがシームレスに協働するサポートワークフローが実現します。\n",
    "\n",
    "環境のセットアップから始めましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Imports that enable human-in-the-loop implementation\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "# Import the AgentCoreMemorySaver that we will use as a checkpointer\n",
    "from langgraph_checkpoint_aws import AgentCoreMemorySaver\n",
    "\n",
    "logging.getLogger(\"support-agent\").setLevel(logging.INFO)\n",
    "region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "\n",
    "logger = logging.getLogger(\"support-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## ステップ 1: メモリの作成\n",
    "このセクションでは、AgentCore Memory SDK を使用してメモリストアを作成します。このメモリは LangGraph チェックポインターのバックエンドとして機能し、永続的な Human-in-the-Loop ワークフローを可能にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_name = \"SupportAgent\"\n",
    "\n",
    "client = MemoryClient(region_name=region)\n",
    "memory = client.create_or_get_memory(name=memory_name)\n",
    "memory_id = memory[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### AgentCore Memory の設定\n",
    "\n",
    "AgentCore Memory チェックポインターを設定し、LLM を初期化しましょう：\n",
    "\n",
    "- `memory_id` はチェックポイントが保存される AgentCore Memory リソースに対応します\n",
    "- `region` はリソースの AWS リージョンを指定します\n",
    "- `MODEL_ID` は LangGraph エージェントを駆動する Bedrock モデルを定義します\n",
    "\n",
    "`memory_id` と追加の boto3 クライアントキーワード引数（この場合は `region`）を使用してチェックポインターをインスタンス化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
    "\n",
    "# Initialize checkpointer for state persistence\n",
    "checkpointer = AgentCoreMemorySaver(memory_id, region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## ステップ 2: Human-in-the-Loop ツール\n",
    "サポートエージェントが使用するツールを定義しましょう。LangGraph の `interrupt` タイプを使用することで、エージェントグラフの実行を中断し、人間が介入してクエリに応答して実行を継続する機会を与えることができます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int):\n",
    "    \"\"\"Add two integers and return the result\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int):\n",
    "    \"\"\"Multiply two integers and return the result\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply, human_assistance]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## ステップ 3: LangGraph エージェントの実装\n",
    "\n",
    "AgentCore Memory チェックポインターと Human-in-the-Loop 機能を使用して LangGraph の `create_react_agent` ビルダーでサポートエージェントを作成しましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=region)\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    prompt=\"You are a helpful assistant\",\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## ステップ 4: サポートエージェントの実行\n",
    "AgentCore Memory チェックポインターと Human-in-the-Loop 統合でエージェントを実行できるようになりました。この例では、明示的にユーザーアシスタンスを要求します。実際には、これは様々な条件でトリガーできます。例えば、特定のキーワードが使用された場合に安全フラグが会話を人間にルーティングすることができます。\n",
    "\n",
    "### 設定のセットアップ\n",
    "LangGraph では、config は呼び出し時に必要な属性（ユーザー ID やセッション ID など）を含む `RuntimeConfig` です。[詳細はこちら](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)を参照してください。\n",
    "\n",
    "AgentCore Memory チェックポインター（`AgentCoreMemorySaver`）では、以下を指定する必要があります：\n",
    "- `thread_id`: AgentCore の session_id にマップ（一意の会話スレッド）\n",
    "- `actor_id`: AgentCore の actor_id にマップ（ユーザー、エージェント、またはその他の識別子）\n",
    "\n",
    "### グラフ呼び出しの入力\n",
    "引数 `inputs` には最新のユーザーメッセージのみを渡す必要があります。これには他の状態変数も含めることができますが、シンプルな `create_react_agent` では、メッセージのみが必要です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I would like to work with a customer service human agent.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"actor_id\": \"demo-notebook\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### ワークフローの中断\n",
    "\n",
    "人間アシスタンスツールが呼び出されたときに実行が一時停止したことに注目してください。現在の状態を検査して、ワークフローがどこで停止したかを確認しましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 人間のスーパーバイザーの介入\n",
    "\n",
    "ここで人間のスーパーバイザーとして行動し、LangGraph `Command` を使用して応答を送信してワークフローを再開するためのアシスタンスを提供しましょう。AgentCore Memory チェックポインターは会話状態全体を保持しており、チャットを再開できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_response = (\n",
    "    \"I'm sorry to hear that you are frustrated. Looking at the past conversation history, I can see that you've requested a refund. I've gone ahead and credited it to your account.\"\n",
    ")\n",
    "\n",
    "human_command = Command(resume={\"messages\": human_response})\n",
    "\n",
    "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、以下を実演しました：\n",
    "\n",
    "1. Human-in-the-Loop ワークフロー用の AgentCore Memory リソースの作成方法\n",
    "2. 中断機能を持つ LangGraph エージェントの構築\n",
    "3. 人間の介入のために実行を一時停止できるツールの実装\n",
    "4. 中断中のワークフロー状態を永続化するための AgentCoreMemorySaver の使用\n",
    "5. 人間が提供したコンテキストでのエージェント実行の再開\n",
    "\n",
    "この統合は、LangGraph の Human-in-the-Loop 機能と AgentCore Memory の堅牢な状態永続化を組み合わせることで、AI エージェントと人間のスーパーバイザーがシームレスに協働する高度なカスタマーサポートワークフローを作成する力を示しています。\n",
    "\n",
    "ここで実演したアプローチは、多段階のエスカレーション、専門的な人間の専門知識ルーティング、複雑な承認ワークフローなど、より複雑なシナリオに拡張できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## クリーンアップ\n",
    "このノートブックで使用したリソースをクリーンアップするためにメモリを削除しましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.delete_memory_and_wait(memory_id = memory_id, max_wait = 300, poll_interval =10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
