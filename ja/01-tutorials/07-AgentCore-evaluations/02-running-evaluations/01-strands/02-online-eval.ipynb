{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## AgentCore Evaluations - Strands エージェントのオンライン評価\n",
    "\n",
    "このチュートリアルでは、Strands エージェントに適用される AgentCore Evaluations のオンライン評価の使用方法を学びます。\n",
    "\n",
    "このラボを実行するには、まず [00-prereqs](../../00-prereqs) フォルダのコードを使用して Strands エージェントを作成し、[01-creating-custom-evaluators](../../01-creating-custom-evaluators) のコードを使用してカスタム評価メトリクスを作成しておく必要があります\n",
    "\n",
    "### 学習内容\n",
    "- AgentCore Starter toolkit を使用してトレースにオンライン評価を実行する方法\n",
    "\n",
    "### チュートリアル詳細\n",
    "\n",
    "| 情報               | 詳細                                                                          |\n",
    "|:-------------------|:------------------------------------------------------------------------------|\n",
    "| チュートリアルタイプ | オンライン評価メトリクス（ビルトインおよびカスタム）を使用した Strands エージェントの評価 |\n",
    "| チュートリアル構成   | ビルトインおよびカスタム評価メトリクスでの自動評価設定                          |\n",
    "| チュートリアル分野   | 横断的                                                                        |\n",
    "| 複雑さ              | 簡単                                                                          |\n",
    "| 使用 SDK            | Amazon Bedrock AgentCore Starter toolkit                                      |\n",
    "\n",
    "### オンライン評価\n",
    "\n",
    "オンライン評価は、デプロイされたエージェントのライブトラフィック品質監視を可能にします。特定の選択されたインタラクションを分析するオンデマンド評価とは異なり、オンライン評価はリアルタイムトラフィックに基づいて本番環境でのエージェントのパフォーマンスを継続的に評価します。\n",
    "\n",
    "オンライン評価は3つの主要なコンポーネントで構成されています。まず、**セッションサンプリングとフィルタリング**により、エージェントインタラクションを評価するための特定のルールを設定できます。すべてのセッションの一部を評価するパーセンテージベースのサンプリング（例：10%）を設定したり、より絞り込んだ評価のための条件フィルタを定義したりできます。次に、新しいカスタム評価メトリクスの作成、既存のカスタム評価メトリクスの使用、またはビルトイン評価メトリクスからの選択を含む**複数の評価方法**から選択できます。最後に、**監視と分析**機能により、ダッシュボードで集計スコアを表示し、時間の経過に伴う品質トレンドを追跡し、低スコアのセッションを調査し、入力から出力までの完全なインタラクションフローを分析できます。\n",
    "\n",
    "オンライン評価では、特定のデータソース（エージェントトレースを含む CloudWatch ロググループまたは AgentCore Runtime エンドポイント）を自動的に監視するようにシステムを設定します。サービスはサンプリングとフィルタリングルールに基づいて受信トレースを継続的に処理し、選択した評価メトリクスをリアルタイムで適用し、分析用の詳細な結果を CloudWatch に出力します。この評価タイプは、本番監視、品質低下の早期検出、ユーザーインタラクションのパターン特定、大規模での一貫したエージェントパフォーマンスの維持に特に便利です。\n",
    "\n",
    "オンライン評価設定を作成して有効にすると、サービスはバックグラウンドで継続的に実行され、セッションが発生するとそれを評価し、エージェントの品質メトリクスへの継続的な可視性を提供します。評価戦略を変更するニーズに合わせて、いつでも設定を一時停止、変更、または削除できます。\n",
    "\n",
    "### AgentCore Observability でエージェントからトレースを生成\n",
    "\n",
    "AgentCore Observability は、[OpenTelemetry (OTEL)](https://opentelemetry.io/) トレースを活用して詳細な実行データをキャプチャし構造化することで、呼び出し中のエージェントの動作を包括的に可視化します。AgentCore は [AWS Distro for OpenTelemetry (ADOT)](https://aws-otel.github.io/) を利用して、さまざまなエージェントフレームワーク間で異なるタイプの OTEL トレースを計装します。\n",
    "\n",
    "エージェントが AgentCore Runtime でホストされている場合（このチュートリアルのエージェントのように）、AgentCore Observability の計装は最小限の設定で自動的に行われます。必要なのは `requirements.txt` に `aws-opentelemetry-distro` を含めることだけで、AgentCore Runtime が OTEL 設定を自動的に処理します。エージェントが AgentCore Runtime で実行されていない場合は、AgentCore Observability で利用できるようにするために ADOT で計装する必要があります。テレメトリデータを CloudWatch に送信するための環境変数を設定し、OpenTelemetry 計装でエージェントを実行する必要があります。\n",
    "\n",
    "プロセスは以下のようになります:\n",
    "\n",
    "![session_traces](../../images/observability_traces.png)\n",
    "\n",
    "セッショントレースが AgentCore Observability で利用可能になると、AgentCore Evaluations を使用してエージェントの動作を評価できます。オンライン評価では、追加の作業は必要ありません。ライブダッシュボードからエージェントのパフォーマンスを監視するだけです。\n",
    "\n",
    "### オンライン評価がトレースとどのように連携するか\n",
    "\n",
    "オンライン評価では、エージェントが呼び出されて AgentCore Observability にトレースが生成されます。これらのトレースはセッションにマッピングされ、ログは Amazon CloudWatch Log グループで利用可能になります。オンライン評価では、開発者が特定のエージェントに対してオンライン評価設定を作成し、サンプルレートとこの設定に適用する評価メトリクスを定義します。AgentCore Evaluations は、設定されたサンプリングレートに従って生成されたトレースを分析し、本番環境でエージェントを自動的に評価します。その後、開発者は AgentCore Observability ダッシュボードを使用してエージェントからのトレースと評価スコアを可視化し、評価結果に基づいてエージェントを継続的に更新できます。\n",
    "\n",
    "\n",
    "![session_traces](../../images/online_evaluations.png)\n",
    "\n",
    "### 前のチュートリアルからの情報の取得\n",
    "\n",
    "このチュートリアルでは、前提条件チュートリアルで AgentCore Runtime にデプロイした Strands エージェントを使用します。事前構築されたメトリクスと、`01-creating-custom-metrics` チュートリアルで作成した `response_quality` メトリクスで評価します。エージェントと評価メトリクスの情報を取得しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r launch_result_strands\n",
    "%store -r evaluator_id\n",
    "try:\n",
    "    print(\"Agent Id:\", launch_result_strands.agent_id)\n",
    "    print(\"Agent ARN:\", launch_result_strands.agent_arn)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing launch results from your Strands agent. Please run 00-prereqs before executing this lab\"\"\")\n",
    "\n",
    "try:\n",
    "    print(\"Evaluator id:\", evaluator_id)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing custom evaluator id. Please run 01-creating-custom-evaluators before executing this lab\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### AgentCore Evaluations クライアントの初期化\n",
    "\n",
    "それでは AgentCore Starter toolkit から AgentCore Evaluations クライアントを初期化しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation, Observability\n",
    "import os\n",
    "import json\n",
    "from boto3.session import Session\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client = Evaluation(region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### オンライン評価設定の構成\n",
    "\n",
    "それではオンライン評価設定を構成しましょう。この場合、デモ目的でのみエージェントを使用しているため、生成されるすべてのトレースを評価します。実際のアプリケーションでは、エージェントの利用状況に応じてサンプルレートを設定する必要があります。\n",
    "\n",
    "オンデマンド評価で探索した5つのメトリクスで評価設定を作成します:\n",
    "* Builtin.GoalSuccessRate\n",
    "* Builtin.Correctness\n",
    "* Builtin.ToolParameterAccuracy\n",
    "* Builtin.ToolSelectionAccuracy および\n",
    "* カスタムメトリクス: response_Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = eval_client.create_online_config(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    config_name=\"strands_agent_eval2\",\n",
    "    sampling_rate=100,\n",
    "    evaluator_list=[\n",
    "        \"Builtin.GoalSuccessRate\", \"Builtin.Correctness\", \n",
    "        \"Builtin.ToolParameterAccuracy\", \"Builtin.ToolSelectionAccuracy\",\n",
    "        evaluator_id\n",
    "    ],\n",
    "    config_description=\"Strands agent online evaluation test\",\n",
    "    auto_create_execution_role=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 評価設定の分析\n",
    "\n",
    "オンライン評価設定の設定 ID を確認しましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Online Evaluation Configuration Id:\", response['onlineEvaluationConfigId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "作成した設定の詳細を確認して、すでに有効になっていることを確認することもできます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client.get_online_config(config_id=response['onlineEvaluationConfigId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### エージェントを呼び出して評価をトリガー\n",
    "\n",
    "オンライン評価をトリガーするために、いくつかの新しいクエリでエージェントを呼び出しましょう。今回はエンドポイントが利用可能になると任意のインターフェースから呼び出せるため、boto3 でエージェントを呼び出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "agentcore_client = boto3.client(\n",
    "    'bedrock-agentcore',\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "def invoke_agent_runtime(agent_arn, prompt):\n",
    "    boto3_response = agentcore_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        qualifier=\"DEFAULT\",\n",
    "        payload=json.dumps({\"prompt\": prompt})\n",
    "    )\n",
    "    if \"text/event-stream\" in boto3_response.get(\"contentType\", \"\"):\n",
    "        content = []\n",
    "        for line in boto3_response[\"response\"].iter_lines(chunk_size=1):\n",
    "            if line:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line.startswith(\"data: \"):\n",
    "                    line = line[6:]\n",
    "                    print(line)\n",
    "                    content.append(line)\n",
    "        display(Markdown(\"\\n\".join(content)))\n",
    "    else:\n",
    "        try:\n",
    "            events = []\n",
    "            for event in boto3_response.get(\"response\", []):\n",
    "                events.append(event)\n",
    "        except Exception as e:\n",
    "            events = [f\"Error reading EventStream: {e}\"]\n",
    "        display(Markdown(json.loads(events[0].decode(\"utf-8\"))))\n",
    "    return boto3_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_strands.agent_arn,\n",
    "    \"How much is 7+9+10*2?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_strands.agent_arn,\n",
    "    \"Is it raining?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_strands.agent_arn,\n",
    "    \"how much is 20% of 300?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_strands.agent_arn,\n",
    "    \"What can you do?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_agent_runtime(\n",
    "    launch_result_strands.agent_arn,\n",
    "    \"What is the capital of NY State?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### オンライン評価の可視化\n",
    "\n",
    "エージェントとの十分なインタラクションを作成すると、[AgentCore Observability コンソール](https://console.aws.amazon.com/cloudwatch/home#gen-ai-observability/agent-core/agents)を使用して、オンライン評価設定に従ってエージェントがどのように動作しているかを可視化できます。\n",
    "\n",
    "エージェントの `DEFAULT` エンドポイントに移動して、現在の評価を確認してください\n",
    "\n",
    "**重要**: 評価結果がダッシュボードに表示されるまで時間がかかる場合があります。評価ダッシュボードが空の場合は、数分待ってから再度確認してください。\n",
    "\n",
    "利用可能になると、エージェントのトレースでメトリクスを直接確認できます:\n",
    "![image.png](../../images/online_evaluations_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### おめでとうございます！\n",
    "\n",
    "最初のオンライン評価設定を作成しました！これで AgentCore Evaluations でカスタムメトリクスを作成し、エージェントをオンデマンドおよびオンラインで評価できます！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
