# 評価の実行

## 概要

このチュートリアルでは、AgentCore Evaluations を使用してオンデマンドとオンライン評価の両方のアプローチでエージェントのパフォーマンスを評価する方法を学びます。組み込みおよびカスタム評価者を適用してエージェントのインタラクションを分析し、大規模な品質監視を行います。

## 学習内容

- 特定のインタラクションをターゲットにした評価のためのオンデマンド評価の実行
- 継続的な本番監視のためのオンライン評価のセットアップ
- エージェント品質を向上させるための評価結果の分析
- 評価の入力として AgentCore Observability トレースを使用

## 前提条件

このチュートリアルを開始する前に、以下を完了している必要があります：
- [チュートリアル 00: 前提条件](../00-prereqs) の完了 - サンプルエージェント（Strands および/または LangGraph）の作成
- [チュートリアル 01: カスタム評価者の作成](../01-creating-custom-evaluators) の完了 - カスタム評価者の作成
- オブザーバビリティを有効にした AgentCore Runtime にデプロイされたエージェント
- AgentCore Observability でトレースを含む少なくとも1つのセッションを生成済み

## 評価タイプ

### オンデマンド評価

オンデマンド評価は、選択したスパン、トレース、またはセッションのセットを直接分析することで、特定のエージェントインタラクションを評価する柔軟な方法を提供します。

**主な特徴：**
- **ターゲット評価**: スパン、トレース、またはセッション ID を提供して特定のインタラクションを評価
- **同期実行**: 評価リクエストに対して即座に結果を取得
- **柔軟なスコープ**: 単一のスパン、完全なトレース、またはセッション全体を評価
- **調査ツール**: 特定の顧客インタラクションの分析や修正の検証に最適

**オンデマンド評価を使用するタイミング：**
- 特定の顧客インタラクションや報告された問題を調査する場合
- 特定された問題の修正を検証する場合
- 品質改善のために過去のデータを分析する場合
- 本番にデプロイする前に評価者をテストする場合
- エッジケースの詳細な分析を行う場合

**仕組み：**

![オンデマンド評価](../images/on_demand_evaluations.png)

1. エージェントが AgentCore Observability でトレースを生成
2. トレースがセッションにマッピングされ、CloudWatch Log グループに保存
3. 評価する特定のセッションまたはトレースを選択
4. 適用する評価者（組み込みまたはカスタム）を指定
5. AgentCore Evaluations が選択したトレースを処理し、詳細な結果を返す

### オンライン評価

オンライン評価は、リアルタイムトラフィックに基づいて本番環境でデプロイされたエージェントの継続的な品質監視を可能にします。

**主な特徴：**
- **継続的な監視**: インタラクションが発生するたびにエージェントのパフォーマンスを自動評価
- **サンプリングベース**: パーセンテージベースのサンプリングまたは条件付きフィルターを設定
- **リアルタイムインサイト**: 品質トレンドを追跡し、リグレッションを早期に検出
- **本番対応**: 最小限のパフォーマンス影響でスケールに対応

**オンライン評価を使用するタイミング：**
- 本番エージェントのパフォーマンスを継続的に監視する場合
- ユーザーに影響を与える前に品質リグレッションを検出する場合
- 大規模なユーザーインタラクションのパターンを特定する場合
- 時間の経過とともに一貫したエージェントパフォーマンスを維持する場合
- 異なるエージェント設定の A/B テストを行う場合

**仕組み：**

![オンライン評価](../images/online_evaluations.png)

1. エージェントが AgentCore Observability でトレースを生成
2. 以下を指定するオンライン評価設定を作成：
   - データソース（CloudWatch ロググループまたは AgentCore Runtime エンドポイント）
   - サンプリングレート（例：全セッションの10%を評価）
   - 適用する評価者（組み込みおよび/またはカスタム）
3. AgentCore Evaluations がルールに基づいて着信トレースを継続的に処理
4. 結果が CloudWatch に出力され、ダッシュボードの可視化と分析が可能
5. 集計スコアを監視し、トレンドを追跡し、低スコアのセッションを調査

## AgentCore Observability 統合

両方の評価タイプは、OpenTelemetry（OTEL）トレースを通じてエージェントの動作をキャプチャするために **AgentCore Observability** に依存しています。

**Observability の仕組み：**

![Observability トレース](../images/observability_traces.png)

AgentCore は、さまざまなエージェントフレームワークにわたって異なるタイプの OTEL トレースを計装するために **AWS Distro for OpenTelemetry（ADOT）** に依存しています：

**AgentCore Runtime でホストされるエージェント**（これらのチュートリアルのエージェントなど）の場合：
- 最小限の設定で自動計装
- `requirements.txt` に `aws-opentelemetry-distro` を含めるだけ
- AgentCore Runtime が OTEL 設定を自動的に処理
- CloudWatch GenAI Observability Dashboard にトレースが表示

**非 Runtime エージェント**の場合：
- CloudWatch にテレメトリを送信するための環境変数を設定
- OpenTelemetry 計装でエージェントを実行
- 詳細は [AgentCore Observability ドキュメント](../../06-AgentCore-observability) を参照

## チュートリアル構成

このチュートリアルでは、AgentCore のフレームワーク非依存機能を示すために **Strands Agents** と **LangGraph** フレームワークの両方の例を提供しています：

### [01-strands](01-strands/)
Strands Agents SDK を使用した例：
- **01-on-demand-eval.ipynb**: 特定のトレースに対するターゲット評価の実行
- **02-online-eval.ipynb**: 継続的な本番監視のセットアップ

### [02-langgraph](02-langgraph/)
LangGraph フレームワークを使用した例：
- **01-on-demand-eval.ipynb**: 特定のトレースに対するターゲット評価の実行
- **02-online-eval.ipynb**: 継続的な本番監視のセットアップ

両方の実装は同じ評価概念を示し、同等の結果を生成することで、AgentCore Evaluations が異なるエージェントフレームワーク間で一貫して動作することを示しています。

## 次のステップ

このチュートリアルを完了した後：
- [チュートリアル 03: 高度](../03-advanced) に進んで、以下の高度な機能を探索：
  - boto3 SDK を使用して CloudWatch ログをクエリしオンデマンド評価
  - 異なるエージェント設定での実験を可視化するローカルダッシュボードの作成
  - オンライン評価のための高度なフィルタリングとサンプリング戦略
