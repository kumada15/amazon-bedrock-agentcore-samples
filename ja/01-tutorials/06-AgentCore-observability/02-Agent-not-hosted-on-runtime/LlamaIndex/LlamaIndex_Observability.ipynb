{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock AgentCore Observability：AgentCore Runtime 外でホストされた LlamaIndex エージェント\n",
    "\n",
    "このノートブックでは、Amazon Bedrock AgentCore Runtime の外部でホストされた [LlamaIndex](https://www.llamaindex.ai/) エージェントの可観測性を設定する方法を説明します。セットアップが完了すると、Amazon CloudWatch の GenAI Observability ダッシュボードで LlamaIndex エージェントの内部意思決定プロセスを確認できるようになります。\n",
    "\n",
    "## 学習内容\n",
    "- Amazon OpenTelemetry Python インストルメンテーションで LlamaIndex エージェントを設定する方法\n",
    "- CloudWatch GenAI Observability でエージェントトレースを可視化および分析する方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 前提条件\n",
    "- Amazon CloudWatch で Transaction Search を有効にしてください。初回ユーザーは Bedrock AgentCore のスパンとトレースを表示するために CloudWatch Transaction Search を有効にする必要があります。Transaction Search を有効にするには、[ドキュメント](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Enable-TransactionSearch.html)を参照してください。\n",
    "- Amazon CloudWatch でロググループとログストリームを設定し、環境変数に追加してください。\n",
    "- Claude Haiku 4.5（モデル ID: global.anthropic.claude-haiku-4-5-20251001-v1:0）への Amazon Bedrock モデルアクセスを持つ AWS アカウント\n",
    "- `aws configure` で AWS 認証情報を設定済み\n",
    "- `LlamaIndex/.env.example` を参考に環境変数を設定した .env ファイル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. セットアップとインストール\n",
    "\n",
    "まず、必要な依存関係をインストールしましょう。requirements.txt ファイルに `aws-opentelemetry-distro` が含まれていることを確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 前提条件のデプロイ\n\n開始する前に、AgentCore 可観測性用のロググループとログストリームを作成しましょう。"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. 環境設定\n",
    "LlamaIndex エージェントの可観測性を有効にし、テレメトリデータを Amazon CloudWatch に送信するには、以下の環境変数を設定する必要があります。`.env` ファイルを使用してこれらの設定を安全に管理し、機密性の高い AWS 認証情報をコードから分離しながら、異なる環境間で簡単に切り替えられるようにします。\n",
    "\n",
    "AWS 認証情報と設定を含む `.env` ファイルを作成してください。テンプレートとして `LlamaIndex/.env.example` を使用してください。\n",
    "\n",
    "既存の `log group` と対応する `log stream` を使用する場合は、それを環境変数に追加してください。\n",
    "\n",
    "そうでない場合は、環境変数として設定する前に、CloudWatch でロググループとログストリームを**作成**する必要があります。サンプル名を提供しています。\n",
    "\n",
    "必要な環境変数:\n",
    "\n",
    "| 変数 | 値 | 目的 |\n",
    "|----------|-------|---------|\n",
    "| `OTEL_PYTHON_DISTRO` | `aws_distro` | AWS Distro for OpenTelemetry (ADOT) を使用 |\n",
    "| `OTEL_PYTHON_CONFIGURATOR` | `aws_configurator` | ADOT SDK 用の AWS コンフィギュレーターを設定 |\n",
    "| `OTEL_EXPORTER_OTLP_PROTOCOL` | `http/protobuf` | エクスポートプロトコルを設定 |\n",
    "| `OTEL_EXPORTER_OTLP_LOGS_HEADERS` | `x-aws-log-group=<YOUR-LOG-GROUP>,x-aws-log-stream=<YOUR-LOG-STREAM>,x-aws-metric-namespace=<YOUR-NAMESPACE>` | ログを CloudWatch グループに送信 |\n",
    "| `OTEL_RESOURCE_ATTRIBUTES` | `service.name=<YOUR-AGENT-NAME>` | 可観測性データでエージェントを識別 |\n",
    "| `AGENT_OBSERVABILITY_ENABLED` | `true` | ADOT パイプラインを有効化 |\n",
    "| `AWS_REGION` | `<YOUR-REGION>` | AWS リージョン |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. 環境変数の読み込み\n",
    "\n",
    "`.env` ファイルから環境変数を読み込みましょう:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Transaction Search の有効化\n\nこの例を実行するには、まず Transaction Search を有効にする必要があります。AWS コンソールでこの[リンク](https://console.aws.amazon.com/cloudwatch/home#xray:settings/transaction-search)から有効化できます。\n\nこのページで、編集をクリックし、OpenTelemetry 形式で構造化ログとしてスパンを取り込むオプションを設定します。\n\n![image.png](./images/transactional_search.png)\n![image.png](./images/transactional_search2.png)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Python ファイルで LlamaIndex エージェントを作成\n",
    "\n",
    "LlamaIndex Travel Agent の実装は `llamaindex_travel_agent.py` に提供されています。Amazon Bedrock のモデルでセットアップされた Travel Agent です。LlamaIndex エージェントは OpenTelemetry でセットアップされ、`opentelemetry-instrument` コマンドを使用する際に AWS OpenTelemetry distro がトレーサープロバイダーのセットアップを自動的に処理します。\n",
    "\n",
    "このエージェントはシンプルな旅行推奨エージェントで、以下を行います:\n",
    "\n",
    "- AWS Bedrock の Claude Haiku モデルを使用して会話型 AI エージェントを作成\n",
    "- 会話フローに LlamaIndex の AgentWorkflow を使用\n",
    "- DuckDuckGo を使用した Web 検索機能を統合\n",
    "- エージェントの決定に基づいてツール呼び出しとルーティングを処理\n",
    "- ユーザークエリに基づいて旅行推奨を返す\n",
    "\n",
    "エージェントは以下で設定されています:\n",
    "\n",
    "- 会話管理用の LlamaIndex Agent ワークフロー\n",
    "- 推論とレスポンス生成のための FunctionCallingAgent\n",
    "- Amazon Bedrock の Claude Haiku モデルを Large Language Model として使用\n",
    "- リアルタイム情報取得のための Web 検索ツール\n",
    "- セッションベースの会話トラッキング\n",
    "\n",
    "`workflow` は、ファイル内で `workflow.run` を使用して作成・実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "# AWS OpenTelemetry Distribution\n",
    "OTEL_PYTHON_DISTRO=aws_distro\n",
    "OTEL_PYTHON_CONFIGURATOR=aws_configurator\n",
    "\n",
    "# Export Protocol\n",
    "OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf\n",
    "OTEL_TRACES_EXPORTER=otlp\n",
    "\n",
    "# CloudWatch Integration (uncomment and configure as needed)\n",
    "OTEL_EXPORTER_OTLP_LOGS_HEADERS=x-aws-log-group=agents/llama-index-agent-logs10,x-aws-log-stream=default,x-aws-metric-namespace=bedrock-agentcore\n",
    "\n",
    "# Service Identification\n",
    "OTEL_RESOURCE_ATTRIBUTES=service.name=agentic-llamaindex-agentcore\n",
    "# Enable Agent Observability\n",
    "AGENT_OBSERVABILITY_ENABLED=true\n",
    "\n",
    "# Disable instrumentations to get rid of span noise (OPTIONAL)\n",
    "OTEL_PYTHON_DISABLED_INSTRUMENTATIONS=jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AWS OpenTelemetry Python Distro\n",
    "\n",
    "環境が設定されたので、可観測性がどのように機能するかを理解しましょう。[AWS OpenTelemetry Python Distro](https://pypi.org/project/aws-opentelemetry-distro/) は、コード変更なしでテレメトリデータをキャプチャするために LlamaIndex エージェントを自動的にインストルメントします。\n",
    "\n",
    "このディストリビューションは以下を提供します:\n",
    "- AgentCore Runtime 外でホストされた LlamaIndex エージェント（EC2、Lambda など）の**自動インストルメンテーション**\n",
    "- シームレスな CloudWatch 統合のための **AWS 最適化設定**\n",
    "\n",
    "### インストルメント済みエージェントの実行\n",
    "\n",
    "LlamaIndex エージェントからトレースをキャプチャするには、Python を直接実行する代わりに `opentelemetry-instrument` コマンドを使用します。これにより、`.env` ファイルの環境変数を使用してインストルメンテーションが自動的に適用されます:\n",
    "\n",
    "```bash\n",
    "opentelemetry-instrument python llamaindex_travel_agent.py\n",
    "```\n",
    "\n",
    "このコマンドは:\n",
    "\n",
    "- .env ファイルから OTEL 設定を読み込みます\n",
    "- LlamaIndex、Amazon Bedrock 呼び出し、エージェントツールとデータベース、およびエージェントが行う他のリクエストを自動的にインストルメントします\n",
    "- トレースを CloudWatch に送信します\n",
    "- GenAI Observability ダッシュボードでエージェントの意思決定プロセスを可視化できるようにします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Display the OTEL-related environment variables\n",
    "otel_vars = [\n",
    "    \"OTEL_PYTHON_DISTRO\",\n",
    "    \"OTEL_PYTHON_CONFIGURATOR\",\n",
    "    \"OTEL_EXPORTER_OTLP_PROTOCOL\",\n",
    "    \"OTEL_EXPORTER_OTLP_LOGS_HEADERS\",\n",
    "    \"OTEL_RESOURCE_ATTRIBUTES\",\n",
    "    \"AGENT_OBSERVABILITY_ENABLED\",\n",
    "    \"OTEL_TRACES_EXPORTER\",\n",
    "    \"OTEL_PYTHON_DISABLED_INSTRUMENTATIONS\"\n",
    "]\n",
    "\n",
    "print(\"OpenTelemetry Configuration:\")\n",
    "for var in otel_vars:\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        print(f\"{var}={value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. セッショントラッキングの追加\n",
    "\n",
    "複数のエージェント実行間でトレースを関連付けるには、OpenTelemetry baggage を使用してセッション ID をテレメトリデータに関連付けることができます:\n",
    "\n",
    "```python\n",
    "from opentelemetry import baggage, context\n",
    "ctx = baggage.set_baggage(\"session.id\", session_id)\n",
    "```\n",
    "\n",
    "セッション対応バージョンを実行:\n",
    "```bash\n",
    "opentelemetry-instrument python llamaindex_travel_agent_with_session.py --session-id \"user-session-123\"\n",
    "```\n",
    "\n",
    "## 7. 分析用カスタムメタデータ\n",
    "フィルタリング、オフライン評価、およびパフォーマンス分析を可能にするためにカスタム属性を追加します。エージェントコードを変更して追加パラメータを受け入れる必要があります:\n",
    "\n",
    "ctx = baggage.set_baggage(\"user.type\", \"premium\")\n",
    "ctx = baggage.set_baggage(\"experiment.id\", \"travel-agent-v2\")\n",
    "ctx = baggage.set_baggage(\"conversation.topic\", \"business-travel\")\n",
    "カスタムメタデータを使用したコマンド例:\n",
    "\n",
    "```bash\n",
    "# 異なる実験の A/B テスト\n",
    "opentelemetry-instrument python agent.py --session-id \"session-123\" --experiment-id \"model-a\"\n",
    "opentelemetry-instrument python agent.py --session-id \"session-124\" --experiment-id \"model-b\"\n",
    "\n",
    "# 異なるユーザータイプのトラッキング\n",
    "opentelemetry-instrument python agent.py --session-id \"session-125\" --user-type \"premium\"\n",
    "opentelemetry-instrument python agent.py --session-id \"session-126\" --user-type \"free\"\n",
    "\n",
    "# オフライン評価実行\n",
    "opentelemetry-instrument python agent.py --session-id \"eval-001\" --dataset \"golden-set-v1\"\n",
    "```\n",
    "\n",
    "これらの属性は CloudWatch トレースに表示され、高度なフィルタリングと分析が可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile llama_index_agent.py\n###########################\n#### エージェントコード: ####\n###########################\nimport os\nimport asyncio\nimport logging\nfrom llama_index.observability.otel import LlamaIndexOpenTelemetry\nfrom llama_index.llms.bedrock_converse import BedrockConverse\nfrom llama_index.core.agent.workflow import FunctionAgent\n\n# LlamaIndex 用の OpenTelemetry インストルメンテーションを初期化\ninstrumentor = LlamaIndexOpenTelemetry(debug=True)\n\n# ロギングの設定\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# LlamaIndex ロギングの設定\nlogging.getLogger(\"llamaindex\").setLevel(logging.INFO)\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"2 つの整数を掛け算し、結果の整数を返します\"\"\"\n    return a * b\n\n\ndef add(a: int, b: int) -> int:\n    \"\"\"2 つの整数を足し算し、結果の整数を返します\"\"\"\n    return a + b\n\n\ndef get_bedrock_model():\n    model_id = os.getenv(\"BEDROCK_MODEL_ID\", \"global.anthropic.claude-haiku-4-5-20251001-v1:0\")\n    region = os.getenv(\"AWS_DEFAULT_REGION\", \"us-west-2\")\n    \n    try:\n        # boto3 に認証情報の解決を自動的に処理させる\n        bedrock_model = BedrockConverse(\n            model=model_id,\n            region_name=region,\n            # 明示的な認証情報なし - boto3 が自動的に見つけます\n        )\n        logger.info(f\"Bedrock モデルを正常に初期化しました: {model_id}、リージョン: {region}\")\n        return bedrock_model\n    except Exception as e:\n        logger.error(f\"Bedrock モデルの初期化に失敗しました: {str(e)}\")\n        logger.error(\"適切な AWS 認証情報が設定されていることと、Bedrock モデルへのアクセス権があることを確認してください\")\n        raise\n\n# モデルを初期化\nbedrock_model = get_bedrock_model()\n\n# 算術エージェントを作成\nagent = FunctionAgent(\n    tools=[add, multiply],\n    llm=bedrock_model,\n)\n\n# リスニングを開始\ninstrumentor.start_registering()\n\n# 算術タスクを実行\nquery = \"\"\"(121 + 2) * 5 は何ですか？\"\"\"\n\nasync def main():\n    result = await agent.run(query)\n    print(\"結果:\", str(result))\n\nasyncio.run(main())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. AWS OpenTelemetry Python Distro\n\n環境が設定されたので、可観測性がどのように機能するかを理解しましょう。[AWS OpenTelemetry Python Distro](https://pypi.org/project/aws-opentelemetry-distro/) は、コード変更なしでテレメトリデータをキャプチャするために LlamaIndex エージェントを自動的にインストルメントします。\n\nこのディストリビューションは以下を提供します:\n- AgentCore Runtime 外でホストされた LlamaIndex エージェント（EC2、Lambda など）の**自動インストルメンテーション**\n- シームレスな CloudWatch 統合のための **AWS 最適化設定**\n\n### インストルメント済みエージェントの実行\n\nLlamaIndex エージェントからトレースをキャプチャするには、Python を直接実行する代わりに `opentelemetry-instrument` コマンドを使用します。これにより、`.env` ファイルの環境変数を使用してインストルメンテーションが自動的に適用されます:\n\n```bash\nopentelemetry-instrument python llama_index_agent.py\n```\n\nこのコマンドは:\n\n- .env ファイルから OTEL 設定を読み込みます\n- LlamaIndex、Amazon Bedrock 呼び出し、エージェントツールとデータベース、およびエージェントが行う他のリクエストを自動的にインストルメントします\n- トレースを CloudWatch に送信します\n- GenAI Observability ダッシュボードでエージェントの意思決定プロセスを可視化できるようにします"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. AWS CloudWatch で GenAI Observability ダッシュボードのトレースを理解する\n",
    "\n",
    "LlamaIndex エージェントが OpenTelemetry インストルメンテーションで実行されると、AWS CloudWatch の GenAI Observability ダッシュボードでトレースを可視化および分析できます。Bedrock Agentcore に移動し、作成したエージェントをクリックします。\n",
    "\n",
    "トレースには以下が表示されます:\n",
    "- **ワークフロー実行**: LlamaIndex ワークフローのステップ\n",
    "- **ツール呼び出し**: Web 検索操作とその結果\n",
    "- **LLM インタラクション**: 入力/出力トークンを含む Bedrock モデル呼び出し\n",
    "- **セッション相関**: セッション ID でグループ化された複数の実行\n",
    "\n",
    "#### セッションページ:\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "#### トレースビューページ:\n",
    "トレースビュー:\n",
    "![image-4.png](attachment:image-4.png)\n",
    "\n",
    "トレース詳細:\n",
    "![image-2.png](attachment:image-2.png)\n",
    "![image-3.png](attachment:image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. トラブルシューティング\n",
    "\n",
    "Amazon CloudWatch でトレースが表示されない場合は、以下を確認してください:\n",
    "\n",
    "1. **AWS 認証情報**: AWS 認証情報が正しく設定されていることを確認\n",
    "2. **IAM 権限**: IAM ユーザー/ロールが CloudWatch の権限を持っていることを確認\n",
    "3. **リージョンとロググループ**: 正しい AWS リージョンとロググループを確認していることを確認\n",
    "4. **環境変数**: すべての OTEL_* 環境変数が正しく設定されていることを確認"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. まとめ\n",
    "\n",
    "おめでとうございます！Amazon CloudWatch を通じて可観測性を持つ Amazon Bedrock モデルで LlamaIndex エージェントを実装してインストルメントしました。\n",
    "\n",
    "- LlamaIndex トラベルエージェント\n",
    "- AWS CloudWatch への完全な OpenTelemetry トレーシング\n",
    "- Amazon Bedrock 呼び出し、LlamaIndex 操作などのトレース"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. 次のステップ\n",
    "\n",
    "OpenTelemetry で LlamaIndex エージェントがセットアップされたので、以下を行うことができます:\n",
    "\n",
    "1. **より複雑なワークフローを構築**: マルチエージェントワークフローを作成\n",
    "2. **エージェントにツールを追加**: 様々な API ツールやカスタムツールを統合\n",
    "3. **[アラームの設定](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)**: `latency`、`token input`、`token output` などのビジネスにとって重要なメトリクスにアラームを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### AWS CloudWatch で GenAI Observability ダッシュボードのトレースを理解する（追加画像）\n\nLlamaIndex エージェントが OpenTelemetry インストルメンテーションで実行されると、AWS CloudWatch の GenAI Observability ダッシュボードでトレースを可視化および分析できます。Bedrock Agentcore に移動し、作成したエージェントをクリックします。\n\n#### セッションビューページ:\n\n![llama_index_sessions.png](images/llama_index_sessions.png)\n\n\n#### トレースビューページ:\nトレースビュー:\n\n![llama_index_sessions.png](images/llama_index_traces.png)\n\n\nトレース詳細:\n\n![llama_index_sessions.png](images/llama_index_trace_details.png)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### トラブルシューティング（追加情報）\n\nAmazon CloudWatch または X-Ray でトレースが表示されない場合は、以下を確認してください:\n\n1. **AWS 認証情報**: AWS 認証情報が正しく設定されていることを確認\n2. **IAM 権限**: IAM ユーザー/ロールが CloudWatch の権限を持っていることを確認\n3. **リージョン**: 正しい AWS リージョンを確認していることを確認\n4. **環境変数**: すべての OTEL_* 環境変数が正しく設定されていることを確認"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### まとめ（追加情報）\n\nおめでとうございます！Amazon CloudWatch を通じて可観測性を持つ Amazon Bedrock モデルで LlamaIndex エージェントを実装してインストルメントしました。\n\n- LlamaIndex 算術エージェント\n- 完全な OpenTelemetry トレーシング\n- Amazon Bedrock 呼び出し、LlamaIndex 操作などのトレース\n- サービス名: agentic-llamaindex-agentcore"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 次のステップ（追加情報）\n\nOpenTelemetry で LlamaIndex エージェントがセットアップされたので、以下を行うことができます:\n\n1. **エージェントの追加**: 異なるパターンでマルチエージェントアーキテクチャを作成\n2. **エージェントにツールを追加**: 検索ツール、API ツール、またはカスタムツールを統合\n3. **[アラームの設定](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)**: `latency`、`token input`、`token output` などのビジネスにとって重要なメトリクスにアラームを作成"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}