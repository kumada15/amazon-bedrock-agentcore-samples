{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth 評価\n",
    "\n",
    "このノートブックは、LLM をジャッジとして使用する拡張可能な LLM ベース評価フレームワークである Strands Evals を使用して、Ground Truth（期待される出力）に対してエージェントの応答を評価します。ルーブリックのみの評価とは異なり、Ground Truth 評価は実際の出力を事前定義された正解と比較します。\n",
    "\n",
    "**ユースケース:**\n",
    "- 回帰テスト: エージェントの変更が既知の良い応答を壊さないことを確認\n",
    "- 品質ベンチマーク: エージェントが期待される動作にどれだけ一致するかを測定\n",
    "- トレーニングデータの検証: キュレートされた例に対してエージェント出力を検証\n",
    "\n",
    "**2つのデータソース（別々のファイル）:**\n",
    "1. **トレースファイル**（`demo_traces.json`）: AgentCore Observability からの実際のエージェント応答を含む\n",
    "2. **Ground Truth ファイル**（`demo_ground_truth.json`）: 各トレースに対するあなたの期待される出力を含む\n",
    "\n",
    "ノートブックは `trace_id` でこれらのファイルをマージして実際と期待を比較します。\n",
    "\n",
    "**2つのモード:**\n",
    "1. **デモモード**: JSON ファイルからサンプルデータを読み込み（AWS アクセス不要）\n",
    "2. **ライブモード**: AgentCore Observability から実際のトレースを取得し、独自の Ground Truth ファイルを提供\n",
    "\n",
    "**このノートブックは2つの評価メトリクスをデモンストレーションします:**\n",
    "- 出力評価: 実際の応答を期待される Ground Truth と比較\n",
    "- トラジェクトリ評価: 実際のツール使用を期待されるツールと比較\n",
    "\n",
    "Strands Evals は、ほぼあらゆる評価タイプのカスタム評価メトリクスをサポートしています。スコアリング基準として表現できる任意の基準を評価するためにこのパターンを拡張できます。\n",
    "\n",
    "**ワークフロー:**\n",
    "1. トレースを読み込み（デモファイルまたは AgentCore Observability からライブ）\n",
    "2. Ground Truth の期待値を読み込み（あなたの期待される出力/トラジェクトリ）\n",
    "3. trace_id でマージ\n",
    "4. 実際 vs 期待を比較する評価メトリクスを実行\n",
    "5. 結果を AgentCore Observability にログ（オプション）\n",
    "6. 結果を分析してギャップを特定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## この位置づけ\n",
    "\n",
    "これは**ノートブック3（オプションB）**です - 期待される Ground Truth 出力と比較してセッションを評価します。\n",
    "\n",
    "![Notebook Workflow](images/notebook_workflow.svg)\n",
    "\n",
    "## Ground Truth 評価の仕組み\n",
    "\n",
    "SME（Subject Matter Expert）が期待される出力を含む Ground Truth ファイルを作成します。これは `trace_id` で実際のトレースとマージされます:\n",
    "\n",
    "![Ground Truth Flow](images/ground_truth_flow.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ\n",
    "\n",
    "モジュールをインポートし、ログを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from strands_evals import Case, Experiment\n",
    "from strands_evals.evaluators import OutputEvaluator, TrajectoryEvaluator\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定\n",
    "\n",
    "デモモード（サンプルファイルを使用）またはライブモード（AgentCore Observability から取得）を選択します。\n",
    "\n",
    "**デモモード**（`USE_DEMO_MODE = True`）:\n",
    "- `DEMO_TRACES_PATH` からトレースを読み込み\n",
    "- `DEMO_GROUND_TRUTH_PATH` から Ground Truth を読み込み\n",
    "- AWS 認証情報不要\n",
    "\n",
    "**ライブモード**（`USE_DEMO_MODE = False`）:\n",
    "- `SESSION_ID` を使用して AgentCore Observability からトレースを取得\n",
    "- `GROUND_TRUTH_PATH` から Ground Truth を読み込み（このファイルを作成する必要があります）\n",
    "- AWS 認証情報と `config.py` 設定が必要\n",
    "\n",
    "**CloudWatch ログ**（`LOG_TO_CLOUDWATCH = True`）:\n",
    "- 評価結果を AgentCore Observability ダッシュボードに送信\n",
    "- `EVALUATION_CONFIG_ID` と評価メトリクス名が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Mode Selection\n",
    "# =============================================================================\n",
    "USE_DEMO_MODE = True\n",
    "\n",
    "# =============================================================================\n",
    "# Demo Mode Paths\n",
    "# =============================================================================\n",
    "DEMO_TRACES_PATH = \"demo_traces.json\"           # Actual agent responses\n",
    "DEMO_GROUND_TRUTH_PATH = \"demo_ground_truth.json\"  # Your expected outputs\n",
    "\n",
    "# =============================================================================\n",
    "# Live Mode Settings\n",
    "# =============================================================================\n",
    "SESSION_ID = \"your-session-id-here\"              # Session to evaluate\n",
    "GROUND_TRUTH_PATH = \"my_ground_truth.json\"       # Your ground truth file\n",
    "\n",
    "# =============================================================================\n",
    "# CloudWatch Logging\n",
    "# =============================================================================\n",
    "LOG_TO_CLOUDWATCH = True\n",
    "OUTPUT_EVALUATOR_NAME = \"Custom.GroundTruthOutput\"\n",
    "TRAJECTORY_EVALUATOR_NAME = \"Custom.GroundTruthTrajectory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファイル形式\n",
    "\n",
    "Ground Truth 評価は、同じ `session_id` を共有する**2つの別々のファイル**を使用します:\n",
    "\n",
    "**主要な概念:**\n",
    "- `session_id`: 単一のユーザーセッションからのすべてのトレースをグループ化\n",
    "- `trace_id`: セッション内の各個別インタラクションを識別\n",
    "\n",
    "### 1. トレースファイル（実際のエージェント応答）\n",
    "エージェントが実際に行ったこと - CloudWatch から取得またはローカルに保存:\n",
    "```json\n",
    "{\n",
    "  \"session_id\": \"5B467129-E54A-4F70-908D-CB31818004B5\",\n",
    "  \"traces\": [\n",
    "    {\n",
    "      \"trace_id\": \"693cb6c4e931\",\n",
    "      \"user_prompt\": \"What is the best route for a NZ road trip?\",\n",
    "      \"actual_output\": \"Based on the search results, here are the best routes...\",\n",
    "      \"actual_trajectory\": [\"web_search\"]\n",
    "    },\n",
    "    {\n",
    "      \"trace_id\": \"693cb6fa87aa\",\n",
    "      \"user_prompt\": \"Should I visit North or South Island?\",\n",
    "      \"actual_output\": \"Here's how the islands compare...\",\n",
    "      \"actual_trajectory\": [\"web_search\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. Ground Truth ファイル（あなたの期待される出力）\n",
    "SME がトレースをレビューし、各 `trace_id` に対して期待される出力を記述:\n",
    "```json\n",
    "{\n",
    "  \"session_id\": \"5B467129-E54A-4F70-908D-CB31818004B5\",\n",
    "  \"ground_truth\": [\n",
    "    {\n",
    "      \"trace_id\": \"693cb6c4e931\",\n",
    "      \"user_prompt_reference\": \"What is the best route for a NZ road trip?\",\n",
    "      \"expected_output\": \"Response should mention Milford Road, Southern Scenic Route...\",\n",
    "      \"expected_trajectory\": [\"web_search\"]\n",
    "    },\n",
    "    {\n",
    "      \"trace_id\": \"693cb6fa87aa\",\n",
    "      \"user_prompt_reference\": \"Should I visit North or South Island?\",\n",
    "      \"expected_output\": \"Response should compare both islands with key features...\",\n",
    "      \"expected_trajectory\": [\"web_search\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**注意:** `user_prompt_reference` はオプションです - SME がどのトレースに対して期待値を記述しているかを覚えておくのに役立ちます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレースと Ground Truth の読み込み\n",
    "\n",
    "デモファイルまたは CloudWatch からトレースデータを読み込み、次に Ground Truth の期待値を読み込み、`trace_id` でマージします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DEMO_MODE:\n",
    "    # Load traces (actual agent responses)\n",
    "    with open(DEMO_TRACES_PATH, \"r\") as f:\n",
    "        traces_data = json.load(f)\n",
    "    \n",
    "    SESSION_ID = traces_data[\"session_id\"]\n",
    "    traces = []\n",
    "    for i, t in enumerate(traces_data[\"traces\"]):\n",
    "        traces.append({\n",
    "            \"trace_index\": i,\n",
    "            \"trace_id\": t.get(\"trace_id\", f\"demo-trace-{i:03d}\"),\n",
    "            \"user_prompt\": t[\"user_prompt\"],\n",
    "            \"actual_output\": t.get(\"actual_output\", \"\"),\n",
    "            \"actual_trajectory\": t.get(\"actual_trajectory\", []),\n",
    "        })\n",
    "    \n",
    "    # Load ground truth (expected outputs) - separate file!\n",
    "    with open(DEMO_GROUND_TRUTH_PATH, \"r\") as f:\n",
    "        gt_data = json.load(f)\n",
    "    \n",
    "    # Build ground truth lookup by trace_id\n",
    "    gt_by_trace_id = {\n",
    "        gt[\"trace_id\"]: {\n",
    "            \"expected_output\": gt[\"expected_output\"],\n",
    "            \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "        }\n",
    "        for gt in gt_data[\"ground_truth\"]\n",
    "    }\n",
    "    \n",
    "    # Merge: match traces to ground truth by trace_id\n",
    "    ground_truth = {}\n",
    "    matched_count = 0\n",
    "    for trace in traces:\n",
    "        trace_id = trace[\"trace_id\"]\n",
    "        if trace_id in gt_by_trace_id:\n",
    "            ground_truth[trace[\"trace_index\"]] = gt_by_trace_id[trace_id]\n",
    "            matched_count += 1\n",
    "    \n",
    "    print(f\"Demo Mode:\")\n",
    "    print(f\"  Traces loaded: {len(traces)} from {DEMO_TRACES_PATH}\")\n",
    "    print(f\"  Ground truth loaded: {len(gt_data['ground_truth'])} entries from {DEMO_GROUND_TRUTH_PATH}\")\n",
    "    print(f\"  Matched by trace_id: {matched_count}\")\n",
    "    print(f\"  Session ID: {SESSION_ID}\")\n",
    "    if gt_data.get(\"description\"):\n",
    "        print(f\"  Description: {gt_data['description']}\")\n",
    "\n",
    "else:\n",
    "    from config import AWS_REGION, SOURCE_LOG_GROUP, LOOKBACK_HOURS\n",
    "    from utils import CloudWatchSessionMapper, ObservabilityClient\n",
    "    from strands_evals.types.trace import AgentInvocationSpan, ToolExecutionSpan\n",
    "    \n",
    "    # Fetch traces from CloudWatch\n",
    "    obs_client = ObservabilityClient(region_name=AWS_REGION, log_group=SOURCE_LOG_GROUP)\n",
    "    mapper = CloudWatchSessionMapper()\n",
    "    \n",
    "    end_time = datetime.now(timezone.utc)\n",
    "    start_time = end_time - timedelta(hours=LOOKBACK_HOURS)\n",
    "    \n",
    "    trace_data = obs_client.get_session_data(\n",
    "        session_id=SESSION_ID,\n",
    "        start_time_ms=int(start_time.timestamp() * 1000),\n",
    "        end_time_ms=int(end_time.timestamp() * 1000),\n",
    "        include_runtime_logs=False,\n",
    "    )\n",
    "    \n",
    "    if not trace_data.spans:\n",
    "        raise ValueError(f\"No spans found for session {SESSION_ID}\")\n",
    "    \n",
    "    session = trace_data.to_session(mapper)\n",
    "    traces = []\n",
    "    for i, trace in enumerate(session.traces):\n",
    "        agent_span = None\n",
    "        tool_calls = []\n",
    "        for span in trace.spans:\n",
    "            if isinstance(span, AgentInvocationSpan):\n",
    "                agent_span = span\n",
    "            elif isinstance(span, ToolExecutionSpan):\n",
    "                tool_calls.append(span.tool_call.name)\n",
    "        if agent_span:\n",
    "            traces.append({\n",
    "                \"trace_index\": i,\n",
    "                \"trace_id\": trace.trace_id,\n",
    "                \"user_prompt\": agent_span.user_prompt or \"\",\n",
    "                \"actual_output\": agent_span.agent_response or \"\",\n",
    "                \"actual_trajectory\": tool_calls,\n",
    "            })\n",
    "    \n",
    "    # Load ground truth from separate file\n",
    "    try:\n",
    "        with open(GROUND_TRUTH_PATH, \"r\") as f:\n",
    "            gt_data = json.load(f)\n",
    "        gt_by_trace_id = {\n",
    "            gt[\"trace_id\"]: {\n",
    "                \"expected_output\": gt[\"expected_output\"],\n",
    "                \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "            }\n",
    "            for gt in gt_data[\"ground_truth\"]\n",
    "        }\n",
    "        ground_truth = {}\n",
    "        for trace in traces:\n",
    "            trace_id = trace[\"trace_id\"]\n",
    "            if trace_id in gt_by_trace_id:\n",
    "                ground_truth[trace[\"trace_index\"]] = gt_by_trace_id[trace_id]\n",
    "        print(f\"Ground truth loaded: {len(ground_truth)} matches from {GROUND_TRUTH_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        ground_truth = {}\n",
    "        print(f\"Ground truth file not found: {GROUND_TRUTH_PATH}\")\n",
    "        print(\"Create a ground truth file or define manually in the next section\")\n",
    "    \n",
    "    print(f\"Live Mode: Loaded {len(traces)} traces from CloudWatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレースのレビュー\n",
    "\n",
    "各トレースの詳細を表示します。各インタラクションの Ground Truth がどのようなものであるべきかを理解するためにこれらをレビューしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in traces:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRACE {trace['trace_index'] + 1} (ID: {trace['trace_id']})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    prompt = trace['user_prompt']\n",
    "    output = trace['actual_output']\n",
    "    \n",
    "    print(f\"\\nUSER PROMPT:\\n{prompt[:500]}...\" if len(prompt) > 500 else f\"\\nUSER PROMPT:\\n{prompt}\")\n",
    "    print(f\"\\nACTUAL OUTPUT:\\n{output[:500]}...\" if len(output) > 500 else f\"\\nACTUAL OUTPUT:\\n{output}\")\n",
    "    print(f\"\\nACTUAL TRAJECTORY: {trace['actual_trajectory']}\")\n",
    "    \n",
    "    if trace['trace_index'] in ground_truth:\n",
    "        gt = ground_truth[trace['trace_index']]\n",
    "        print(f\"\\nEXPECTED OUTPUT: {gt['expected_output']}\")\n",
    "        print(f\"EXPECTED TRAJECTORY: {gt['expected_trajectory']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth の定義（ライブモードのみ）\n",
    "\n",
    "ライブモードを使用している場合、ここで各トレースの期待される出力と期待されるトラジェクトリを定義します。\n",
    "\n",
    "**デモモードでは、Ground Truth は JSON ファイルから事前に読み込まれています。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ground_truth:\n",
    "    print(\"Ground truth not defined. Generating template...\\n\")\n",
    "    print(\"Copy, edit, and paste the following:\\n\")\n",
    "    print(\"ground_truth = {\")\n",
    "    for trace in traces:\n",
    "        prompt_preview = trace['user_prompt'][:50].replace('\"', \"'\")\n",
    "        print(f\"    {trace['trace_index']}: {{\")\n",
    "        print(f'        \"expected_output\": \"TODO: {prompt_preview}...\",') \n",
    "        print(f'        \"expected_trajectory\": {trace[\"actual_trajectory\"]},')\n",
    "        print(f\"    }},\")\n",
    "    print(\"}\")\n",
    "else:\n",
    "    print(f\"Ground truth already defined for {len(ground_truth)} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価メトリクスのルーブリック\n",
    "\n",
    "両方の評価メトリクスはルーブリックを使用して実際 vs 期待を比較します:\n",
    "\n",
    "- **OutputEvaluator**: 意味的類似性を使用して実際の応答を期待される出力と比較\n",
    "- **TrajectoryEvaluator**: 実際のツールトラジェクトリを期待されるトラジェクトリと比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_output_rubric = \"\"\"\n",
    "Compare the agent's actual output against the expected ground truth output.\n",
    "\n",
    "Evaluation criteria:\n",
    "1. Semantic Match (0-0.5): Does the actual output convey the same meaning as expected?\n",
    "   - 0.5: Full semantic alignment - same information and intent\n",
    "   - 0.3: Partial alignment - captures main points but misses details\n",
    "   - 0.0: No alignment - different information or wrong answer\n",
    "\n",
    "2. Completeness (0-0.3): Does the actual output include all key points from expected?\n",
    "   - 0.3: All key information present\n",
    "   - 0.15: Most key information present\n",
    "   - 0.0: Missing critical information\n",
    "\n",
    "3. Correctness (0-0.2): Is the actual output factually consistent with expected?\n",
    "   - 0.2: No factual contradictions\n",
    "   - 0.1: Minor inconsistencies\n",
    "   - 0.0: Major contradictions or errors\n",
    "\n",
    "Final score = sum of all criteria (0.0 to 1.0)\n",
    "\"\"\"\n",
    "\n",
    "trajectory_rubric = \"\"\"\n",
    "Compare the agent's actual tool trajectory against the expected trajectory.\n",
    "\n",
    "Evaluation criteria:\n",
    "1. Tool Match (0-0.5): Did the agent use the expected tools?\n",
    "   - 0.5: All expected tools were used\n",
    "   - 0.25: Some expected tools were used\n",
    "   - 0.0: None of the expected tools were used\n",
    "\n",
    "2. No Extra Tools (0-0.3): Did the agent avoid unnecessary tools?\n",
    "   - 0.3: No extra tools beyond expected\n",
    "   - 0.15: One extra tool\n",
    "   - 0.0: Multiple unnecessary tools\n",
    "\n",
    "3. Order (0-0.2): Were tools used in the expected sequence?\n",
    "   - 0.2: Correct order\n",
    "   - 0.1: Minor order differences\n",
    "   - 0.0: Completely different order\n",
    "\n",
    "Final score = sum of all criteria (0.0 to 1.0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価ケースの作成\n",
    "\n",
    "比較のために実際の出力と期待される Ground Truth の両方を含む Cases を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_cases(traces: List[Dict], ground_truth: Dict[int, Dict], session_id: str) -> List[Case]:\n",
    "    \"\"\"Create evaluation cases with ground truth for comparison.\"\"\"\n",
    "    cases = []\n",
    "    \n",
    "    for trace in traces:\n",
    "        idx = trace[\"trace_index\"]\n",
    "        gt = ground_truth.get(idx, {})\n",
    "        \n",
    "        if not gt:\n",
    "            print(f\"Warning: No ground truth for trace {idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        case = Case(\n",
    "            name=f\"trace_{idx}_{trace['trace_id'][:8]}\",\n",
    "            input=trace[\"user_prompt\"],\n",
    "            expected_output=gt.get(\"expected_output\", \"\"),\n",
    "            session_id=session_id,\n",
    "            metadata={\n",
    "                \"actual_output\": trace[\"actual_output\"],\n",
    "                \"actual_trajectory\": trace[\"actual_trajectory\"],\n",
    "                \"expected_trajectory\": gt.get(\"expected_trajectory\", []),\n",
    "                \"trace_id\": trace[\"trace_id\"],\n",
    "            },\n",
    "        )\n",
    "        cases.append(case)\n",
    "    \n",
    "    return cases\n",
    "\n",
    "cases = create_ground_truth_cases(traces, ground_truth, SESSION_ID)\n",
    "print(f\"Created {len(cases)} evaluation cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## タスク関数\n",
    "\n",
    "これらの関数は、評価メトリクスが比較するための実際と期待の値を抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_task_fn(case: Case) -> str:\n",
    "    \"\"\"Return actual and expected output for comparison.\"\"\"\n",
    "    actual = case.metadata.get(\"actual_output\", \"\")\n",
    "    expected = case.expected_output or \"\"\n",
    "    return f\"ACTUAL OUTPUT:\\n{actual}\\n\\nEXPECTED OUTPUT (Ground Truth):\\n{expected}\"\n",
    "\n",
    "\n",
    "def trajectory_task_fn(case: Case):\n",
    "    \"\"\"Return output and trajectory for TrajectoryEvaluator.\n",
    "    \n",
    "    TrajectoryEvaluator expects a dictionary with 'output' and 'trajectory' keys.\n",
    "    \"\"\"\n",
    "    actual_output = case.metadata.get(\"actual_output\", \"\")\n",
    "    actual_trajectory = case.metadata.get(\"actual_trajectory\", [])\n",
    "    expected_trajectory = case.metadata.get(\"expected_trajectory\", [])\n",
    "    \n",
    "    # Format output to include comparison context\n",
    "    comparison_output = f\"\"\"ACTUAL OUTPUT:\n",
    "{actual_output}\n",
    "\n",
    "EXPECTED TRAJECTORY: {expected_trajectory}\n",
    "ACTUAL TRAJECTORY: {actual_trajectory}\"\"\"\n",
    "    \n",
    "    # Return dictionary format expected by TrajectoryEvaluator\n",
    "    return {\"output\": comparison_output, \"trajectory\": actual_trajectory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth 評価の実行\n",
    "\n",
    "実際の出力を Ground Truth と比較する評価を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"Running Output Evaluation...\")\n",
    "    output_evaluator = OutputEvaluator(rubric=ground_truth_output_rubric)\n",
    "    output_experiment = Experiment(cases=cases, evaluators=[output_evaluator])\n",
    "    output_results = output_experiment.run_evaluations(ground_truth_task_fn)\n",
    "    output_report = output_results[0]  # Extract the report from the list\n",
    "    print(f\"Output Evaluation Complete - Overall Score: {output_report.overall_score:.2f}\")\n",
    "else:\n",
    "    print(\"No cases to evaluate. Please define ground truth first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"Running Trajectory Evaluation...\")\n",
    "    \n",
    "    # Collect all unique tools from actual and expected trajectories\n",
    "    all_tools = set()\n",
    "    for trace in traces:\n",
    "        all_tools.update(trace[\"actual_trajectory\"])\n",
    "    for gt in ground_truth.values():\n",
    "        all_tools.update(gt.get(\"expected_trajectory\", []))\n",
    "    \n",
    "    trajectory_evaluator = TrajectoryEvaluator(\n",
    "        rubric=trajectory_rubric,\n",
    "        trajectory_description={\"available_tools\": list(all_tools)}\n",
    "    )\n",
    "    trajectory_experiment = Experiment(cases=cases, evaluators=[trajectory_evaluator])\n",
    "    trajectory_results = trajectory_experiment.run_evaluations(trajectory_task_fn)\n",
    "    trajectory_report = trajectory_results[0]\n",
    "    print(f\"Trajectory Evaluation Complete - Overall Score: {trajectory_report.overall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詳細結果\n",
    "\n",
    "トレースごとのスコアと説明を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GROUND TRUTH EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        print(f\"\\n--- Trace {i+1}: {case.name} ---\")\n",
    "        prompt_display = case.input[:80] + \"...\" if len(case.input) > 80 else case.input\n",
    "        print(f\"User Prompt: {prompt_display}\")\n",
    "        \n",
    "        output_score = output_report.scores[i] if i < len(output_report.scores) else 0\n",
    "        output_reason = output_report.reasons[i] if i < len(output_report.reasons) else \"N/A\"\n",
    "        print(f\"\\nOutput Score: {output_score:.2f}\")\n",
    "        print(f\"Explanation: {output_reason[:250]}...\" if len(str(output_reason)) > 250 else f\"Explanation: {output_reason}\")\n",
    "        \n",
    "        traj_score = trajectory_report.scores[i] if i < len(trajectory_report.scores) else 0\n",
    "        traj_reason = trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"N/A\"\n",
    "        print(f\"\\nTrajectory Score: {traj_score:.2f}\")\n",
    "        print(f\"Expected Tools: {case.metadata.get('expected_trajectory', [])}\")\n",
    "        print(f\"Actual Tools:   {case.metadata.get('actual_trajectory', [])}\")\n",
    "        print(f\"Explanation: {traj_reason[:250]}...\" if len(str(traj_reason)) > 250 else f\"Explanation: {traj_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudWatch への結果ログ（オプション）\n",
    "\n",
    "元のトレース ID を使用して評価結果を AgentCore Observability ダッシュボードに送信します。これにより、AgentCore Observability コンソールで元のトレースと一緒に Ground Truth 評価スコアを確認できます。\n",
    "\n",
    "この機能を有効にするには、設定セクションで `LOG_TO_CLOUDWATCH = True` に設定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_CLOUDWATCH and cases:\n",
    "    from config import EVALUATION_CONFIG_ID, setup_cloudwatch_environment\n",
    "    from utils import send_evaluation_to_cloudwatch\n",
    "    \n",
    "    # Setup CloudWatch environment\n",
    "    setup_cloudwatch_environment()\n",
    "    \n",
    "    output_logged = 0\n",
    "    trajectory_logged = 0\n",
    "    \n",
    "    print(\"Logging results to CloudWatch...\")\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        trace_id = case.metadata.get(\"trace_id\", \"\")\n",
    "        if not trace_id:\n",
    "            continue\n",
    "        \n",
    "        # Log output evaluation result\n",
    "        output_score = output_report.scores[i] if i < len(output_report.scores) else 0\n",
    "        output_reason = output_report.reasons[i] if i < len(output_report.reasons) else \"\"\n",
    "        if send_evaluation_to_cloudwatch(\n",
    "            trace_id=trace_id,\n",
    "            session_id=SESSION_ID,\n",
    "            evaluator_name=OUTPUT_EVALUATOR_NAME,\n",
    "            score=output_score,\n",
    "            explanation=str(output_reason)[:500],\n",
    "            config_id=EVALUATION_CONFIG_ID,\n",
    "        ):\n",
    "            output_logged += 1\n",
    "        \n",
    "        # Log trajectory evaluation result\n",
    "        traj_score = trajectory_report.scores[i] if i < len(trajectory_report.scores) else 0\n",
    "        traj_reason = trajectory_report.reasons[i] if i < len(trajectory_report.reasons) else \"\"\n",
    "        if send_evaluation_to_cloudwatch(\n",
    "            trace_id=trace_id,\n",
    "            session_id=SESSION_ID,\n",
    "            evaluator_name=TRAJECTORY_EVALUATOR_NAME,\n",
    "            score=traj_score,\n",
    "            explanation=str(traj_reason)[:500],\n",
    "            config_id=EVALUATION_CONFIG_ID,\n",
    "        ):\n",
    "            trajectory_logged += 1\n",
    "    \n",
    "    print(f\"CloudWatch logging complete:\")\n",
    "    print(f\"  Output evaluations logged: {output_logged}/{len(cases)}\")\n",
    "    print(f\"  Trajectory evaluations logged: {trajectory_logged}/{len(cases)}\")\n",
    "else:\n",
    "    if not LOG_TO_CLOUDWATCH:\n",
    "        print(\"CloudWatch logging disabled. Set LOG_TO_CLOUDWATCH = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サマリー\n",
    "\n",
    "エージェントが Ground Truth にどれだけ一致したかを示す集計結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSession: {SESSION_ID}\")\n",
    "    print(f\"Mode: {'Demo' if USE_DEMO_MODE else 'Live'}\")\n",
    "    print(f\"Traces Evaluated: {len(cases)}\")\n",
    "    \n",
    "    print(f\"\\nOutput Evaluation (Actual vs Expected Response):\")\n",
    "    print(f\"  Overall Score: {output_report.overall_score:.2f}\")\n",
    "    print(f\"  Range: {min(output_report.scores):.2f} - {max(output_report.scores):.2f}\")\n",
    "    \n",
    "    print(f\"\\nTrajectory Evaluation (Actual vs Expected Tools):\")\n",
    "    print(f\"  Overall Score: {trajectory_report.overall_score:.2f}\")\n",
    "    print(f\"  Range: {min(trajectory_report.scores):.2f} - {max(trajectory_report.scores):.2f}\")\n",
    "    \n",
    "    low_output = [i+1 for i, s in enumerate(output_report.scores) if s < 0.5]\n",
    "    low_traj = [i+1 for i, s in enumerate(trajectory_report.scores) if s < 0.5]\n",
    "    \n",
    "    if low_output:\n",
    "        print(f\"\\nTraces with low output scores (<0.5): {low_output}\")\n",
    "    if low_traj:\n",
    "        print(f\"Traces with low trajectory scores (<0.5): {low_traj}\")\n",
    "    \n",
    "    if not low_output and not low_traj:\n",
    "        print(f\"\\nAll traces scored above 0.5 - agent behavior matches ground truth well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果のエクスポート\n",
    "\n",
    "評価結果を JSON に保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cases:\n",
    "    export_data = {\n",
    "        \"evaluation_time\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"session_id\": SESSION_ID,\n",
    "        \"mode\": \"demo\" if USE_DEMO_MODE else \"live\",\n",
    "        \"evaluation_type\": \"ground_truth\",\n",
    "        \"summary\": {\n",
    "            \"traces_evaluated\": len(cases),\n",
    "            \"output_overall_score\": output_report.overall_score,\n",
    "            \"trajectory_overall_score\": trajectory_report.overall_score,\n",
    "        },\n",
    "        \"traces\": [\n",
    "            {\n",
    "                \"trace_id\": case.metadata.get(\"trace_id\"),\n",
    "                \"user_prompt\": case.input,\n",
    "                \"expected_output\": case.expected_output,\n",
    "                \"actual_output\": case.metadata.get(\"actual_output\"),\n",
    "                \"expected_trajectory\": case.metadata.get(\"expected_trajectory\"),\n",
    "                \"actual_trajectory\": case.metadata.get(\"actual_trajectory\"),\n",
    "                \"output_score\": output_report.scores[i] if i < len(output_report.scores) else None,\n",
    "                \"trajectory_score\": trajectory_report.scores[i] if i < len(trajectory_report.scores) else None,\n",
    "            }\n",
    "            for i, case in enumerate(cases)\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    output_path = \"ground_truth_results.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Results exported to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
