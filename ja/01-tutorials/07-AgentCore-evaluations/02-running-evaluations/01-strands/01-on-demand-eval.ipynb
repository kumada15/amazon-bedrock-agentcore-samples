{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## AgentCore Evaluations - Strands エージェントのオンデマンド評価\n",
    "\n",
    "このチュートリアルでは、Strands エージェントに適用される AgentCore Evaluations のオンデマンド評価の使用方法を学びます。\n",
    "\n",
    "このラボを実行するには、まず [00-prereqs](../../00-prereqs) フォルダのコードを使用して Strands エージェントを作成し、[01-creating-custom-evaluators](../../01-creating-custom-evaluators) のコードを使用してカスタム評価メトリクスを作成しておく必要があります\n",
    "\n",
    "### 学習内容\n",
    "- AgentCore Starter toolkit を使用してトレースにオンデマンド評価を実行する方法\n",
    "\n",
    "### チュートリアル詳細\n",
    "\n",
    "| 情報               | 詳細                                                                          |\n",
    "|:-------------------|:------------------------------------------------------------------------------|\n",
    "| チュートリアルタイプ | オンデマンド評価メトリクス（ビルトインおよびカスタム）を使用した Strands エージェントの評価 |\n",
    "| チュートリアル構成   | ビルトインおよびカスタム評価メトリクスでの評価実行                               |\n",
    "| チュートリアル分野   | 横断的                                                                        |\n",
    "| 複雑さ              | 簡単                                                                          |\n",
    "| 使用 SDK            | Amazon Bedrock AgentCore Starter toolkit                                      |\n",
    "\n",
    "### オンデマンド評価\n",
    "\n",
    "オンデマンド評価は、選択したスパンのセットを直接分析することで、特定のエージェントインタラクションを柔軟に評価する方法を提供します。本番トラフィックを継続的に監視するオンライン評価とは異なり、オンデマンド評価では選択したインタラクションに対してターゲットを絞った評価をいつでも実行できます。\n",
    "\n",
    "オンデマンド評価では、スパン、トレース、またはセッション ID を提供することで、評価したい正確なスパン、トレース、またはセッションを指定します。AgentCore Starter toolkit を使用すると、セッション内のすべてのトレースを自動的に評価することもできます。\n",
    "\n",
    "カスタム評価メトリクスまたはビルトイン評価メトリクスをエージェントのインタラクションに適用できます。この評価タイプは、特定の顧客インタラクションを調査する必要がある場合、報告された問題の修正を検証する場合、または品質改善のために履歴データを分析する場合に特に便利です。評価リクエストを送信すると、サービスは指定されたスパンのみを処理し、分析用の詳細な結果を提供します。\n",
    "\n",
    "### AgentCore Observability でエージェントからトレースを生成\n",
    "\n",
    "AgentCore Observability は、[OpenTelemetry (OTEL)](https://opentelemetry.io/) トレースを活用して詳細な実行データをキャプチャし構造化することで、呼び出し中のエージェントの動作を包括的に可視化します。AgentCore は [AWS Distro for OpenTelemetry (ADOT)](https://aws-otel.github.io/) を利用して、さまざまなエージェントフレームワーク間で異なるタイプの OTEL トレースを計装します。\n",
    "\n",
    "エージェントが AgentCore Runtime でホストされている場合（このチュートリアルのエージェントのように）、AgentCore Observability の計装は最小限の設定で自動的に行われます。必要なのは `requirements.txt` に `aws-opentelemetry-distro` を含めることだけで、AgentCore Runtime が OTEL 設定を自動的に処理します。エージェントが AgentCore Runtime で実行されていない場合は、AgentCore Observability で利用できるようにするために ADOT で計装する必要があります。テレメトリデータを CloudWatch に送信するための環境変数を設定し、OpenTelemetry 計装でエージェントを実行する必要があります。\n",
    "\n",
    "プロセスは以下のようになります:\n",
    "\n",
    "![session_traces](../../images/observability_traces.png)\n",
    "\n",
    "セッショントレースが AgentCore Observability で利用可能になると、AgentCore Evaluations を使用してエージェントの動作を評価できます。\n",
    "\n",
    "### オンデマンド評価がトレースとどのように連携するか\n",
    "\n",
    "オンデマンド評価では、エージェントが呼び出されて AgentCore Observability にトレースが生成されます。これらのトレースはセッションにマッピングされ、ログは Amazon CloudWatch Log グループで利用可能になります。オンデマンド評価では、開発者がどのセッションまたはトレースを使用するかを決定し、トレースの内容を評価するメトリクスとともに AgentCore Evaluations に入力として送信します。プロセスは以下のようになります:\n",
    "\n",
    "\n",
    "![session_traces](../../images/on_demand_evaluations.png)\n",
    "\n",
    "### 前のチュートリアルからの情報の取得\n",
    "\n",
    "このチュートリアルでは、前提条件チュートリアルで AgentCore Runtime にデプロイした Strands エージェントを使用します。事前構築されたメトリクスと、`01-creating-custom-metrics` チュートリアルで作成した `response_quality` メトリクスで評価します。エージェントと評価メトリクスの情報を取得しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r launch_result_strands\n",
    "%store -r session_id_strands\n",
    "%store -r evaluator_id\n",
    "try:\n",
    "    print(\"Agent Id:\", launch_result_strands.agent_id)\n",
    "    print(\"Agent ARN:\", launch_result_strands.agent_arn)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing launch results from your Strands agent. Please run 00-prereqs before executing this lab\"\"\")\n",
    "\n",
    "try:\n",
    "    print(\"Session id:\", session_id_strands)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing session id from your Strands agent. Please run 00-prereqs before executing this lab\"\"\")\n",
    "\n",
    "try:\n",
    "    print(\"Evaluator id:\", evaluator_id)\n",
    "except NameError as e:\n",
    "    raise Exception(\"\"\"Missing custom evaluator id. Please run 01-creating-custom-evaluators before executing this lab\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### AgentCore Evaluations クライアントの初期化\n",
    "\n",
    "それでは AgentCore Starter toolkit から AgentCore Evaluations クライアントを初期化しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation, Observability\n",
    "import os\n",
    "import json\n",
    "from boto3.session import Session\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client = Evaluation(region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 評価の実行\n",
    "\n",
    "AgentCore Evaluations を実行するには、セッション、トレース、またはスパン情報を提供する必要があります。前のチュートリアルで見たように、異なるメトリクスはエージェントトレースから異なるレベルの情報を必要とします。\n",
    "\n",
    "![metrics level](../../images/metrics_per_level.png)\n",
    "\n",
    "AWS の SDK のいずれかを使用している場合、トレースを自分で処理する必要があります。AgentCore Starter toolkit はこのプロセスを簡素化し、セッション ID またはトレース ID に基づいてトレースを処理します。これにより、`run` インタラクション中に AgentCore starter toolkit 評価クライアントにセッション ID を提供すると、SDK がそのセッション内のすべてのトレースを抽出し、トレースおよびスパンレベルのメトリクスで評価します。オプションで、評価結果を保存するための出力ファイル名も提供できます。\n",
    "\n",
    "### Goal Success Rate\n",
    "\n",
    "エージェントの Goal Success Rate を評価しましょう。エージェントには以下の質問をしたことを覚えておいてください:\n",
    "\n",
    "* What is the weather now?\n",
    "* How much is 2+2?\n",
    "* Can you tell me the capital of the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_sucess_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[\"Builtin.GoalSuccessRate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "評価メトリクスの結果を理解しましょう。結果オブジェクトには、実行された評価についての情報（session_id、trace_id、input_data）と評価結果が含まれています。\n",
    "\n",
    "評価結果には、評価メトリクス情報（id、name、ARN）、評価値、評価ラベル、評価説明、評価ジョブに関する追加のコンテキスト（spanContext、token_usage など）が含まれます。\n",
    "\n",
    "評価レスポンスを見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in goal_sucess_results.results:\n",
    "    information = f\"\"\"\n",
    "    Goal Success: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "同じセッションをトレースレベルのメトリクス Correctness で分析しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[\"Builtin.Correctness\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "評価メトリクスの結果を理解しましょう。この場合、Correctness はトレースレベルで評価されるため、各トレースはそれぞれ独自の評価結果を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in correctness_results.results:\n",
    "    information = f\"\"\"\n",
    "    Correctness: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Tool selection accuracy と parameter selection accuracy\n",
    "\n",
    "ツールとパラメータ選択についてエージェントを評価しましょう。両方のメトリクスはスパンレベルで評価されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[\"Builtin.ToolParameterAccuracy\", \"Builtin.ToolSelectionAccuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "結果を分析しましょう。この場合、同じ run で2つの異なるメトリクスでセッションを評価しています。つまり、どの評価メトリクスが各レスポンスを生成しているかを知る必要があります。これは結果の `evaluator_name` プロパティで確認できます。エージェントがツールをどれだけうまく使用したか見てみましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in parameter_results.results:\n",
    "    information = f\"\"\"\n",
    "    Metric: {result.evaluator_name}\n",
    "    Value: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### カスタム評価メトリクスの使用\n",
    "\n",
    "セッション、トレース、スパンレベルで評価メトリクスを使用したので、カスタムメトリクスを使用して応答品質を評価しましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[evaluator_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "評価結果を見てみましょう。この場合、以下の指示を持つエージェントを評価しています:\n",
    "\n",
    "```\n",
    "You're a helpful assistant. You can do simple math calculation, and tell the weather.\n",
    "```\n",
    "\n",
    "評価メトリクスでは、評価指示に記載されているように、スコープ外の応答に対して `Very Poor` の品質でエージェントにペナルティを与えています:\n",
    "\n",
    "```\n",
    "...\n",
    "**IMPORTANT**: A response quality can only be high if the agent remains in its original scope. Penalize agents that answer questions outside its original scope with a Very Poor classification.\n",
    "...\n",
    "```\n",
    "\n",
    "以下の質問を評価しているため:\n",
    "\n",
    "* What is the weather now?\n",
    "* How much is 2+2?\n",
    "* Can you tell me the capital of the US?\n",
    "\n",
    "最後の質問に対してエージェントが `Very Poor` の評価を受けることを期待しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in custom_results.results:\n",
    "    information = f\"\"\"\n",
    "    Metric: {result.evaluator_name}\n",
    "    Value: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    display(Markdown(information))\n",
    "    print(\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 評価結果の保存\n",
    "\n",
    "AgentCore starter toolkit は、構造化された出力ファイルにエージェント評価の結果を保存するのにも役立ちます。run 中に `ouput` パラメータを提供するだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = eval_client.run(\n",
    "    agent_id=launch_result_strands.agent_id,\n",
    "    session_id=session_id_strands, \n",
    "    evaluators=[\n",
    "        evaluator_id\n",
    "    ],\n",
    "    output=\"evals_results/output.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### おめでとうございます！\n",
    "\n",
    "オンデマンド機能でエージェントを評価しました。次のチュートリアルでは、オンライン評価メトリクスを設定してエージェントに接続することで、本番環境でのエージェント評価を自動化します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
