{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph と AgentCore Memory Hooks（長期記憶）\n",
    "\n",
    "## はじめに\n",
    "\n",
    "このノートブックでは、LangGraph フレームワークを使用して Amazon Bedrock AgentCore Memory 機能を会話型 AI エージェントに統合する方法を紹介します。**長期記憶**の保持に焦点を当て、エージェントが過去のインタラクションからユーザーの好み、食事制限、コンテキスト情報を抽出して想起できるようにします。\n",
    "\n",
    "## チュートリアルの詳細\n",
    "\n",
    "| 項目               | 詳細                                                                             |\n",
    "|:-------------------|:---------------------------------------------------------------------------------|\n",
    "| チュートリアルタイプ | 長期記憶型会話                                                                   |\n",
    "| エージェントユースケース | 栄養アシスタント                                                              |\n",
    "| エージェントフレームワーク | LangGraph                                                                    |\n",
    "| LLM モデル          | Anthropic Claude Haiku 4.5                                                      |\n",
    "| チュートリアルコンポーネント | AgentCore 長期 Memory、カスタム Memory 戦略、Pre/Post モデルフック          |\n",
    "| 難易度              | 中級                                                                             |\n",
    "\n",
    "学習内容：\n",
    "- UserPreference カスタムオーバーライド戦略を使用した AgentCore Memory の作成\n",
    "- 自動メモリ保存と取得のための pre/post モデルフックの実装\n",
    "- セッション間でユーザーの好みを記憶する栄養アシスタントの構築\n",
    "- 関連するユーザーコンテキストを取得するためのセマンティック検索の使用\n",
    "- カスタムメモリ抽出と統合プロンプトの設定\n",
    "\n",
    "### シナリオの背景\n",
    "\n",
    "この例では、食事制限、お気に入りの食べ物、調理の好み、健康目標などのユーザーコンテキストを複数の会話にわたって記憶できる**栄養アシスタント**を作成します。エージェントは会話からユーザーの好みを自動的に抽出して保存し、将来のインタラクションで関連するコンテキストを取得してパーソナライズされた栄養アドバイスを提供します。\n",
    "\n",
    "## アーキテクチャ\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"architecture.png\" width=\"65%\" />\n",
    "</div>\n",
    "\n",
    "## 前提条件\n",
    "\n",
    "- Python 3.10以上\n",
    "- 適切な権限を持つ AWS アカウント\n",
    "- AgentCore Memory の適切な権限を持つ AWS IAM ロール\n",
    "- Amazon Bedrock モデルへのアクセス\n",
    "\n",
    "環境のセットアップから始めましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries from https://github.com/langchain-ai/langchain-aws\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "import uuid\n",
    "\n",
    "\n",
    "region = os.getenv('AWS_REGION', 'us-east-1')\n",
    "logging.getLogger(\"math-agent\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AgentCoreMemoryStore that we will use as a store\n",
    "from langgraph_checkpoint_aws import (\n",
    "    AgentCoreMemoryStore\n",
    ")\n",
    "\n",
    "# For this example, we will just use an InMemorySaver to save context.\n",
    "# In production, we highly recommend the AgentCoreMemorySaver as a checkpointer which works seamlessly alongside the memory store\n",
    "#from langgraph_checkpoint_aws import AgentCoreMemorySaver\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "from bedrock_agentcore.memory.constants import StrategyType\n",
    "\n",
    "from custom_memory_prompts import consolidation_prompt, extraction_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_name = \"NutritionAssistant\"\n",
    "client = MemoryClient(region_name=region)\n",
    "MODEL_ID = \"global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
    "\n",
    "memory = client.create_or_get_memory(\n",
    "    name=memory_name,\n",
    "    description=\"Nutrition assistant\",\n",
    "    memory_execution_role_arn=\"arn:aws:iam::YOUR_ACCOUNT:role/YOUR_ROLE\", # Please provide a role with a valid trust policy\n",
    "    strategies=[\n",
    "        {\n",
    "            StrategyType.CUSTOM.value: {\n",
    "                \"name\": \"NutritionPreferences\",\n",
    "                \"description\": \"Captures customer food preferences and behavior\",\n",
    "                \"namespaces\": [\"/{actorId}/preferences\"],\n",
    "                \"configuration\": {\n",
    "                    \"userPreferenceOverride\": {\n",
    "                        \"extraction\": {\n",
    "                            \"appendToPrompt\": extraction_prompt,\n",
    "                            \"modelId\": MODEL_ID,\n",
    "                        },\n",
    "                        \"consolidation\": {\n",
    "                            \"appendToPrompt\": consolidation_prompt,\n",
    "                            \"modelId\": MODEL_ID,\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "memory_id = memory[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory 設定の概要\n",
    "\n",
    "AgentCore Memory のセットアップには以下が含まれます：\n",
    "\n",
    "- **Custom Strategy**: 会話から栄養の好みを抽出\n",
    "- **Namespaces**: ユーザーごとにメモリを整理（`{actorId}/preferences`）\n",
    "- **Custom Prompts**: 食べ物の好み用の特殊化された抽出と統合ロジック\n",
    "- **Model Integration**: メモリ処理に Claude 3.7 Sonnet を使用\n",
    "\n",
    "メモリシステムは、一時的または無関係な情報をフィルタリングしながら、永続的なユーザーの好みを抽出するために会話を自動的に処理します。\n",
    "\n",
    "## ステップ 3: Memory Store と LLM の初期化\n",
    "\n",
    "次に、AgentCore Memory Store と言語モデルを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the store to enable long term memory saving and retrieval\n",
    "store = AgentCoreMemoryStore(memory_id=memory_id, region_name=region)\n",
    "\n",
    "# Initialize Bedrock LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ 4: Memory Hooks の実装\n",
    "\n",
    "メモリの保存と取得を自動的に処理する pre および post モデルフックを作成します：\n",
    "\n",
    "- **Pre-model hook**: 関連するユーザーの好み（セマンティック検索に基づく）を取得し、LLM 呼び出し前にコンテキストを追加\n",
    "- **Post-model hook**: 長期記憶抽出のために会話メッセージを保存\n",
    "\n",
    "### Memory 処理の仕組み\n",
    "\n",
    "1. メッセージは actor_id と session_id とともに AgentCore Memory に保存されます\n",
    "2. カスタム戦略が会話を処理して栄養の好みを抽出します\n",
    "3. 抽出された好みは `{actorId}/preferences` 名前空間に保存されます\n",
    "4. 将来の会話でコンテキストのために関連する好みを検索・取得できます\n",
    "\n",
    "**注意**: LangChain のメッセージタイプは、ストアによって内部で AgentCore Memory のメッセージタイプに変換されるため、長期記憶に適切に抽出できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model_hook(state, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Hook that runs pre-LLM invocation to save the latest human message\"\"\"\n",
    "    actor_id = config[\"configurable\"][\"actor_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "    # Saving the message to the actor and session combination that we get at runtime\n",
    "    namespace = (actor_id, thread_id)\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    # Save the last human message we see before LLM invocation\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n",
    "            break\n",
    "    # Retrieve user preferences based on the last message and append to state\n",
    "    user_preferences_namespace = (actor_id, \"preferences\")\n",
    "    preferences = store.search(user_preferences_namespace, query=msg.content, limit=5)\n",
    "    \n",
    "    # Construct another AI message to add context before the current message\n",
    "    if preferences:\n",
    "        context_items = [pref.value for pref in preferences]\n",
    "        context_message = AIMessage(\n",
    "            content=f\"[User Context: {', '.join(str(item) for item in context_items)}]\"\n",
    "        )\n",
    "        # Insert the context message before the last human message\n",
    "        return {\"messages\": messages[:-1] + [context_message, messages[-1]]}\n",
    "    \n",
    "    return {\"llm_input_messages\": messages}\n",
    "\n",
    "def post_model_hook(state, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Hook that runs post-LLM invocation to save the latest human message\"\"\"\n",
    "    actor_id = config[\"configurable\"][\"actor_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "\n",
    "    # Saving the message to the actor and session combination that we get at runtime\n",
    "    namespace = (actor_id, thread_id)\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    # Save the LLMs response to AgentCore Memory\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n",
    "            break\n",
    "    \n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ 5: LangGraph Agent の作成\n",
    "\n",
    "メモリフックを統合した LangGraph の `create_react_agent` を使用して栄養アシスタントエージェントを作成します。ツールノードには長期記憶取得ツールのみが含まれ、pre および post モデルフックが引数として指定されます。\n",
    "\n",
    "**注意**: カスタムエージェント実装の場合、Store とツールは任意のワークフローでこのパターンに従って必要に応じて設定できます。Pre/post モデルフックを使用することも、会話全体を最後に保存することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_react_agent(\n",
    "    llm,\n",
    "    store=store,\n",
    "    tools=[], # No additional tools needed for this example\n",
    "    checkpointer=InMemorySaver(), # For conversation state management\n",
    "    pre_model_hook=pre_model_hook, # Retrieves user preferences before LLM call\n",
    "    post_model_hook=post_model_hook  # Saves conversation after LLM response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ 6: Agent ランタイムの設定\n",
    "\n",
    "ユーザーとセッションの一意の識別子でエージェントを設定する必要があります。これらの ID はメモリの整理と取得に不可欠です。\n",
    "\n",
    "### Graph の入力\n",
    "`inputs` 引数として最新のユーザーメッセージを渡すだけで済みます。他の状態変数も含めることができますが、シンプルな `create_react_agent` では messages のみが必要です。\n",
    "\n",
    "### LangGraph RuntimeConfig\n",
    "LangGraph では、config は呼び出し時に必要な属性（ユーザー ID やセッション ID など）を含む `RuntimeConfig` です。`AgentCoreMemorySaver` の場合、`thread_id` と `actor_id` を config に設定する必要があります。例えば、AgentCore の呼び出しエンドポイントは、呼び出し元の ID またはユーザー ID に基づいてこれを割り当てることができます。追加の[ドキュメントはこちら](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)で確認できます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_id = \"user-1\"\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-1\", # REQUIRED: This maps to Bedrock AgentCore session_id under the hood\n",
    "        \"actor_id\": actor_id, # REQUIRED: This maps to Bedrock AgentCore actor_id under the hood\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ 7: Agent のテスト\n",
    "\n",
    "食べ物の好みについての会話で栄養アシスタントをテストしましょう。エージェントは将来の使用のためにユーザーの好みを自動的に抽出して保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pretty print agent output while running\n",
    "def run_agent(query: str, config: RunnableConfig):\n",
    "    printed_ids = set()\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                # Check if we've already printed this message\n",
    "                if id(msg) not in printed_ids:\n",
    "                    msg.pretty_print()\n",
    "                    printed_ids.add(id(msg))\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Hey there! Im cooking one of my favorite meals tonight, salmon with rice and veggies (healthy). Has\n",
    "great macros for my weightlifting competition that is coming up. What can I add to this dish to make it taste better\n",
    "and also improve the protein and vitamins I get?\n",
    "\"\"\"\n",
    "\n",
    "run_agent(prompt, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 何が保存されたか？\n",
    "ご覧のとおり、モデルはまだ私たちの好みや食事制限について何も把握していません。\n",
    "\n",
    "pre/post モデルフックを使用したこの実装では、2つのメッセージが保存されました。ユーザーからの最初のメッセージと AI モデルからの応答の両方が AgentCore Memory に会話イベントとして保存されました。長期記憶が抽出されるまでに数秒かかる場合があるため、最初の試行で何も見つからない場合は数秒後に再試行してください。\n",
    "\n",
    "これらのメッセージは、ファクトとユーザー設定の名前空間で AgentCore の長期記憶に抽出されました。実際、これまでに保存された内容をストア自体で確認できます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search our user preferences namespace\nsearch_namespace = (actor_id, \"preferences\")\nresult = store.search(search_namespace, query=\"food\", limit=3)\nprint(f\"プリファレンス名前空間の結果: {result}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store へのエージェントアクセス\n",
    "\n",
    "**注意** - AgentCore Memory はこれらのイベントをバックグラウンドで処理するため、メモリが抽出されて長期記憶取得用に埋め込まれるまでに数秒かかる場合があります。\n",
    "\n",
    "素晴らしい！会話の以前のメッセージに基づいて、長期記憶が名前空間に抽出されたことが確認できました。\n",
    "\n",
    "次に、新しいセッションを開始して、夕食に何を作るべきかについての推薦を求めてみましょう。エージェントはストアを使用して抽出された長期記憶にアクセスし、ユーザーが気に入る推薦を行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-2\", # New session ID\n",
    "        \"actor_id\": actor_id, # Same actor ID\n",
    "    }\n",
    "}\n",
    "\n",
    "run_agent(\"Today's a new day, what should I make for dinner tonight?\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### まとめ\n",
    "\n",
    "ご覧のとおり、エージェントはユーザー設定名前空間の検索から pre-model hook コンテキストを受け取り、ファクト名前空間の長期記憶を自分で検索して、ユーザーのために包括的な回答を作成することができました。\n",
    "\n",
    "AgentCoreMemoryStore は非常に柔軟で、pre/post モデルフックやストア操作を持つツール自体など、さまざまな方法で実装できます。チェックポイント用の AgentCoreMemorySaver と一緒に使用することで、完全な会話状態と長期的なインサイトの両方を組み合わせて、複雑でインテリジェントなエージェントシステムを形成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}